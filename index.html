<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating Pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies">
  <meta name="keywords" content="Pi0, Robotics, VLA, Vision-Language-Action, PAL, GRASP Lab">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Evaluating Pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://grasp.upenn.edu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://grasp.upenn.edu/">
            GRASP Lab
          </a>
          <a class="navbar-item" href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">
            PAL Research Group
          </a>
          <a class="navbar-item" href="https://www.cis.upenn.edu/~kostas/">
            Daniilidis Research Group
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Evaluating Pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://everloom-129.github.io/">Jie Wang</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.grasp.upenn.edu/people/matthew-leonard/">Matthew Leonard</a>,</span>
            <span class="author-block">
              <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,</span>
            <span class="author-block">
              <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>,</span>
            <span class="author-block">
              <a href="https://edwardshu.com/">Edward S. Hu</a> 
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">GRASP Lab, University of Pennsylvania</span>
          </div>
          <!-- TODO ADD formal note -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">Published:June 1, 2025</span> 
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">*Corresponding author: tonyw3@seas.upenn.edu</span> 
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper links -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2501.09747" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Pi0-FAST Paper</span>
                </a>
              </span> -->
              <!-- Paper links -->
              <span class="link-block">
                <a href="https://www.physicalintelligence.company/blog" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>PI Research Papers</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://droid-dataset.github.io/droid/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>DROID Dataset</span>
                </a>
              </span>

              <!-- GitHub link -->
              <span class="link-block">
                <a href="https://www.physicalintelligence.company/blog" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>OpenPI Github Repo</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="static/videos/success/success_novel_fold_newspaper.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        "Fold up the newspaper"
      </h2>
    </div>
  </div>
</section>



<!-- TODO for Edward -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Pi0 is a state-of-the-art vision-language-action (VLA) model designed for general-purpose robotic manipulation, trained on internet-scale data and the Open X-Embodiment dataset. While Pi0 promises versatility across tasks like pick-and-place, articulation, dexterity and human robot interaction, we ask: how does it perform when released in the wild?
          </p>
          <p>
            Just like how users rank LLMs by chatting about arbitrary topics in <a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard">Chatbot Arena</a>, we subject Pi0 to "vibe checks" through unstructured real-world experimentation. Users interact with the policies on a whim - improvising tasks, altering camera angles, rearranging objects, and probing edge cases. By embracing the high entropy of ad-hoc human testing, we stress-test the policy's generalization capabilities, while discovering interesting phenomena that structured evaluations may miss.
          </p>
          <p>
            Critically, vibe-based evaluation only becomes trustworthy when people can easily try out and verify the models themselves. We found the setup of Pi0 to be very close to the experience of using open-source LLMs today - download the checkpoint and deploy, with no additional environment-specific instrumentation required (e.g. camera calibration, controller tuning, demo collection, etc.). We summarize our discoveries over 300+ trials of Pi0 below, exploring its strengths, limitations, unexpected quirks, and its implications for the future of robot learning. We hope that it inspires others to try out Pi0 for themselves!
          </p>
          <p>
            <strong>Key words: Evaluation, pi0, VLAs, Manipulation, Robot Learning, Imitation Learning. </strong> 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Box -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="box has-background-light">
          <h3 class="title is-4 has-text-centered">TL;DR</h3>
          <div class="content">
            <ul class="is-size-5">
              <li><strong>Overall Success:</strong> 42.3% average task completion across 300+ trials</li>
              <li><strong>Key Finding:</strong> Pi0 demonstrates impressive vision-language understanding but struggles with spatial reasoning and precise manipulation</li>
              <li><strong>Bottom Line:</strong> Pi0 works impressively in the wild without fine-tuning, but success depends heavily on prompt engineering and object familiarity</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TODO for Edward: phrase Bottom Line, the statement is weird -->

<!-- Teaser Images Section -->
<section class="hero">
  <div class="container" style="width: 50%; max-width: 50%;">
    <div class="hero-body" style="padding: 1.5rem 1.5rem;">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <!-- First row - three images side by side -->
            <div class="columns is-vcentered is-gapless" style="margin-bottom: 1rem;">
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/lab_setup.jpg" alt="GRASP Lab Setup" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
    </div>
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/robot_workspace.jpg" alt="Robot Workspace" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
  </div>
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/lab_setup_2.jpg" alt="Levine Setup 2" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
              </div>
            </div>
            
            <!-- Second row - manipulation scene (full width) -->
            <div class="columns is-centered" style="margin-top: 0.5rem;">
              <div class="column is-12">
                <figure class="image">
                  <img src="static/images/wild-newsppaper.png" alt="Three Camera View" style="width: 100%;">
                </figure>
              </div>
            </div>
            
            <p class="has-text-centered" style="margin-top: 1rem;">
              Our evaluation setup at GRASP Lab, showing the robot workspace, test objects, and typical manipulation scenes.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Key Findings Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2. Key Findings</h2>
        <div class="content has-text-justified">
          <p>
            Through experiments across diverse manipulation tasks in the <a href="https://facilities.upenn.edu/maps/locations/levine-hall-melvin-and-claire-weiss-tech-house">Levine Hall</a> 403 & 457 at Penn Engineering School, we observed a wide range of behaviors—remarkable performances, confusing failures, and many quirks. By testing Pi0 in varied environments (e.g., cluttered tables, articulated cabinets, human-involved settings), we have derived three critical insights:
          </p>

          <ol>
            <li>
              <strong>Pi0 Works (Mostly)</strong>: It achieves <strong>42.3% average progress</strong> across diverse, hard tasks, even in unseen environments and objects. However, it also fails in some seemingly simple tasks.
            </li>
            <li>
              <strong>Prompt Engineering Matters</strong>: While Pi0 is good at vision-action grounding, its performance drops <strong>20~100%</strong> based on instruction phrasing. You need to carefully prompt it to make it work in some condition.
            </li>
            <li>
              <strong>Unexpected Quirks</strong>: Pi0 can recover from failures, and handle moving humans in the scene, but struggles with <strong>mid-task freezing, collision avoidance, and fine-grained manipulation etc.</strong>
            </li>
          </ol>
          
          <p>
            Below, we unpack each finding with video examples and analysis.
          </p>

          <h3 class="title is-4">Evaluation: Progress Score through Vibe Testing</h3>
          
          <p>
            Evaluating large generalist robot policies like <strong>pi0</strong> remains an open challenge. Unlike classical robotics benchmarks, these models operate across diverse tasks and environments, often in open-ended settings with no binary notion of success.
          </p>

          <p>
            In our blog, we adopt a <strong>progress score</strong> as the main evaluation signal, which is <strong>loosely inspired by the "vibe testing" rubric</strong> proposed in the <a href="https://arxiv.org/abs/2403.04132">ChatbotArena(Chiang, etc, 2024)</a>, (<a href="https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation">Lambert 2024</a>). This method has also been used in recent works such as the pi0 and pi0-FAST papers. During evaluation, we assign a continuous score from 0 to 100 to each policy rollout, reflecting the <strong>fraction of the commanded task successfully completed</strong>. For instance, a <strong>newspaper-folding</strong> task might score:
          </p>

          <ul>
            <li><strong>25</strong>: if the robot approached and grasp a side of newspaper.</li>
            <li><strong>50</strong>: if it fold up the newspaper in correct articulation direction.</li>
            <li><strong>75</strong>: if the newspaper is partially fold up.</li>
            <li><strong>100</strong>: if the newspaper is fold neatly in half.</li>
          </ul>

          <p>
            These scores are inherently subjective and noisy, but practical for generalist policy evaluation: we try to remain <strong>consistent across trials</strong>, and use them as <strong>noisy but informative approximations of task completion</strong>.
          </p>

          <p>
            This scalar scoring allows us to quickly assess success modes, failure modes, and emerging capabilities across the experiments. With the development of robotics evaluation, we believe more standardized, benchmark-driven evaluation methods for generalist policies will emerge. For now, <strong>vibe-informed progress scores</strong> are a reasonable and transparent compromise.
          </p>

          <p>
            For more details, see <a href="#appendix">Appendix B: Results</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Hero Images -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <div class="columns">
            <div class="column is-half">
              <img src="static/images/OverallResult.png" alt="Scatter plot showing performance distribution">
            <figcaption style="text-align: center;">Overall result of each task type. The left is the relationship between score and number of trials, the right is the progress rate on each task. </figcaption>
          </div>
            <div class="column is-half">
              <img src="static/images/worldcloud.png" alt="Word cloud visualization">
            <figcaption style="text-align: center;">Word cloud of task instructions</figcaption>
        </div>

          </div>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Disclaimer Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="box has-background-warning-light">
          <h3 class="title is-4">Disclaimer</h3>
          <div class="content has-text-justified">
            <p>
              Our evaluations were conducted using the <strong>π₀-FAST-DROID</strong> model, specifically on <strong>DROID robot setups</strong>. While the model supports prompt-based control, its <strong><i>"LLM-like-download-and-prompt"</i></strong> functionality is currently limited to <strong>embodiments similar to those seen during training</strong>—particularly setups included in the <strong>Distributed RObot Interaction Dataset (DROID)</strong>.
            </p>
            <p> 
              The evaluation is conducted from Feb. 22 to March.22, with a focus on limited environment and tasks type. Please check the Appendix section for more details on results. 
            </p>
            <p>
              PAL Research Group also contributed data to the DROID dataset in late 2023, making our setup more in-distribution. However, our setup has changed significantly since late 2023 - signficant environmental changes like camera positions, robot height and calibration, new objects, lighting, etc.All results, observations, and analyses presented in this blog are based solely on  <strong>our own rollouts</strong>, and <strong>do not reflect any official performance</strong> guarantees from the model authors or dataset maintainers.
            </p>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Success Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Success Cases</h2>

        <!-- Success videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Pick and Place</h3>
                  <div>
                    <video id="success-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_pickplace_fish_to_box.mp4" type="video/mp4">
      </video>
    </div>
                  <p class="is-size-6">"Place the yellow fish into the purple box"</p>
                  <p class="is-size-6">Precise placement of camouflage fish into the box</p>
  </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Articulation</h3>
                  <div>
                    <video id="success-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_articulation_open_drawer.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Open the drawer"</p>
                  <p class="is-size-6">Opening drawer with multiple pull to make sure fully opened</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Human Robot Interaction</h3>
                  <div>
                    <video id="success-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_hri_handover_pineapple.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Hand the pineapple to the programmer"</p>
                  <p class="is-size-6">Safe object handover to programmer, even there is wire occlusion from side view</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Dexterity</h3>
                  <div>
                    <video id="success-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_dexterity_pour_water.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pour water from the silver cup to the pink bowl"</p>
                  <p class="is-size-6">Pour real water from Latte art vat into the target bowl</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Multi-step Task</h3>
                  <div>
                    <video id="success-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_multistep_fill_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pick up all the objects into the basket"</p>
                  <p class="is-size-6">Sequential bagging all the toys into basket</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Novel Objects</h3>
                  <div>
                    <video id="success-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/right_Close_the_capsule_lid_of_the_coffee_machine_left.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Close the capsule lid of the coffee machine"</p>
                  <p class="is-size-6">Handling previously unseen device</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Remarkable Performance Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">3. Remarkable Performance of PI0</h2>

        <!-- Vision-Language Understanding -->
        <h3 class="title is-4">3.1 Robust Vision-Language Understanding in Complex Scenes</h3>
        <div class="content has-text-justified">
          <p>
            Powered by <strong>PaliGemma</strong> (Google DeepMind's 3B VLM) as its vision encoder (<a href="https://arxiv.org/abs/2407.07726">Beyer etc, 2024</a>), Pi0 demonstrates robust scene comprehension and adaptability. Despite relying solely on uncalibrated monocular RGB inputs (224x224 pixels after compression), it can handle very challenging objects and environments like transparent, camouflaged and novel items. Showing the potential of End2End VLA in precision, closed loop control.
          </p>
          
          <p><strong>1. It can grasp transparent objects</strong></p>
          
          <p>
            PI0 is capable of identifying and manipulating transparent objects, as shown below. It picks up the bottle with a stable grasp, aligns it to the small cup, and precisely drops it in. Many traditional grasp detection techniques require an accurate 2D or 3D reconstruction of the scene, and transparent objects can cause issues in reconstruction accuracy. This makes it all the more impressive that the model can detect transparent objects solely from uncalibrated, mono-RGB views from the wrist camera and one side view camera.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\Success_Dropping_Into_Cup.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the white cup."</p>
        </div>
      </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\transparent_bottle_place.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the bowl."</p>
        </div>
      </div>
          </div>

          <p><strong>2. It can grasp an object even when it is camouflaged into a colorful background</strong></p>
          
          <p>
            PI0 can identify the 'yellow fish' here even when it is placed on top of a colorful board game. This object has an unusual and difficult shape, and it blends in well with the background, but pi0 detects well with its position, and its shape, well enough to grasp it up.
          </p>
          
    <div class="columns is-centered">
            <div class="column is-half"> 
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/success/camflage_yellow_fish.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the red box"</p>
          </div>
        </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_pickplace_fish_to_box.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the purple box"</p>
      </div>
    </div>
  </div>
          
          <p><strong>3. It is robust to human activity in the input</strong></p>
          
          <p>
            During evaluation, there were many times where the side-view camera captured humans moving around in the background. However, pi0 can always focus on its task, keeping the robotic arm's movements focused on object manipulation.
          </p>
          
          <p>
            We believe there are two reasons for pi0's robustness to human movement. First, the pre-trained VLM backbone of Pi0 is trained on human-involved images (<a href="https://aclanthology.org/P18-1238/">Sharma et al., 2018</a>, <a href="https://aclanthology.org/2022.naacl-main.142/">Changpinyo et al., 2022a</a>, <a href="https://storage.googleapis.com/openimages/web/factsfigures_v7.html">Kuznetsova et al. 2020</a>), so humans are in-distribution. Next, as our occlusion experiments in <strong>Section 2.3.1</strong>, the policy seems to prioritize the wrist camera's images during pick-and-place tasks, so distractors in the side-view camera seem to minimally affect the policy.
          </p>
          
          <p>
            Here are two side-view videos involving humans in the scene. Please refer to <strong>Appendix B.6</strong> for more experiments on <strong>human-robot interaction</strong>.
          </p>

          <p><em>(All videos featuring humans were uploaded with permission from the individuals involved.)</em></p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_initial_PI0_pineapple.mp4" type="video/mp4">
      </video>
                <p class="is-size-6 has-text-centered">"Pick the pineapple and place it into the basket"</p>
    </div>
  </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_hand_the_pineapple_to_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to the outstretched hand"</p>
              </div>
            </div>
          </div>
            

          <p>
            There's plenty of work in the past in CV, Robotics that does transparent object detection and manipulation. But the nice part here is that we have a data-driven system that does it, without any special logic or care for transparent objects.
          </p>
          <p class="box has-background-grey-lighter has-text-centered is-italic">
            "Pi0's ability to handle transparency, clutter and distractors hints at a future where robots see the world as humans do—through semantics, not just pixels."
          </p>
          
        </div>
        
        <!-- Behavioral Structure: pi0 is an FSM -->
        <h3 class="title is-4">3.2 Behavioral Structure: pi0 Exhibits FSM-like Patterns</h3>
        <div class="content has-text-justified">
          <p>
            Through our experiments, we observed that <strong>pi0 exhibits behavior resembling a Finite State Machine (FSM)</strong> across a wide range of manipulation tasks. Despite being an <strong>autoregressive model without explicit memory or state encoding</strong>, pi0 often transitions through intuitive behavioral phases such as:
          </p>
          
          <p style="text-align: center;">
            <strong>Search → Reach → Grasp → Transfer → Release → Reset → Idle</strong>
          </p>

          <p>
            What's remarkable is that this structure <strong>emerges naturally from the data</strong> rather than being explicitly modeled — suggesting that pi0 learns consistent <strong>task execution priors</strong> across environments. For instance, even when pi0 is unfamiliar with an object or task, it often <strong>proactively explores</strong> near affordance-rich areas using its wrist camera to decide whether to grasp or not.
          </p>

          <p>
            In certain trials, we also observed <strong>reset-like behaviors</strong>: if pi0 perceives the task as complete (e.g., after placing an item into a bowl), it may <strong>return to its home configuration and idle</strong>. While this often indicates a well-formed task boundary, it can also lead to <strong>early stopping /freeze issues</strong>, especially in multi-object scenes — see <strong>Section 4.1</strong> for analysis of early stopping failure cases.
          </p>

          <p>
            <em>While this FSM-like sequencing might suggest an implicit temporal abstraction mechanism, we caution against framing pi0 having 'memory' or 'history'. These patterns may reflect properties of the task distribution (e.g., Markovian, short-horizon tasks), rather than indicating explicit task inference or memory.</em>
          </p>

          <!-- Existing video grid -->
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_remove_the_pink_bowl_from_the_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Remove the pink bowl from the tray"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_stack_the_wooden_blocks.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Stack the wooden blocks"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_fold_the_cloth_from_left_to_right.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold the cloth from left to right"</p>
              </div>
            </div>
          </div>
          
          <p class="box has-background-grey-lighter has-text-centered is-italic">
            "Pi0's FSM pattern indicates it is a strong meta learning algorithm being able to generalize across environments with pretrained data."
          </p>
        </div>
        
        <!-- Failure Recovery -->
        <!-- <h3 class="title is-4">3.3 Failure recovery and generalization</h3>
        <div class="content has-text-justified">
          <p>
            One key difference between the pi0 and other open source VLA models are: it can work in the wild, without online data fine-tuning, without calibration, without much position tuning on the policy, we can let it run on our franka robot directly.
          </p>
          
          <p>
            More interestingly, the VLA behaves similar to diffusion policy, as shown below, it could recover from failures and retry tasks that initially failed.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\3.3place_the_pen_into_pen_holder.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the pen into pen holder"</p>
              </div>
              <div>
            </div>
     
          </div>
          </div>

          <div class="columns is-multiline is-centered">
            <div class="column is-full">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\3.3_fold_up_the_newspaper_from_right_side.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold up the newspaper from left side"</p>
              </div>
            </div>
          </div>
          


        </div> -->
      </div>

      
    </div>
  </div>
</section>

<!-- Failiure Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Failture Case</h2>
        <p class="has-text-justified" style="color: #666; font-style: italic;">
          Pi0 demonstrates very impressive robustness across different lights, locations and tasks. However, it do exist some weakness, here is some examples on its cons:
        </p>
        <!-- Failure videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Objects</h3>
                  <div>
                    <video id="failure-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\4_failure_pour_the_water_from_teapot_into_bowl.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour water from teapot into bowl"</i></p>
                  <p class="is-size-6">Cannot manipulate a new glass teapot (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Background</h3>
                  <div>
                    <video id="failure-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\upenn_pick_block.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pick the black box on the white box"</i></p>
                  <p class="is-size-6">Can not handle well with unseen background (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Task Misunderstanding</h3>
                  <div>
                    <video id="failure-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_fish_into_the_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"place the yellow fish into the basket"</i></p>
                  <p class="is-size-6">Pick up the wrong object in cluttered scene (25% success rate)</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Spatial Reasoning</h3>
                  <div>
                    <video id="failure-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_can_into_the_tray.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Place the can into the tray"</i></p>
                  <p class="is-size-6">Misjudges object position relative to container (30% success rate)</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">General Articulation</h3>
                  <div>
                    <video id="failure-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\video_close_the_right_cabinet_door.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Close the right cabinet door"</i></p>
                  <p class="is-size-6">Fails to open toy kitchen cabinet on the table (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Coffee Making</h3>
                  <div>
                    <video id="failure-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\coffee_pour_coffee_bean_into_grinder.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour coffee bean into the grinder"</i></p>
                  <p class="is-size-6">Cannot work with espresso machine (0% success rate)</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Problems with PI0 Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">4. Problem with PI0</h2>

        <!-- Early stopping problem -->
        <h3 class="title is-4">4.1 Early stopping</h3>
        <div class="content has-text-justified">
          <p>
            One common failure case is that policy may <strong>freeze unexpectedly during execution</strong>, leaving tasks incomplete until human intervention. 
            This behavior comes from two interrelated factors: semantic ambiguity and autoregressive action decoding limitations.
          </p>


          <!-- 3 video examples of early stopping -->
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_hand_the_pineapple_to_human.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to human"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_open the drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Open the drawer"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_pour coffee bean into the coffee grinder.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Pour coffee bean into the coffee grinder"</p>
              </div>
            </div>
          </div>


          
          <h4 class="title is-5">Possible Causes</h4>
          
          <p><strong>1. VLA doesn't understand the instruction</strong></p>
          <p>
            Pi0 lacks LLM-like commonsense reasoning to infer unfamiliar object categories. When it does not understand a command, it gets stuck. 
            In some experiments, we found that some objects / instructions are out of distribution(OOD), causing early stopping.
          </p>
          
          
          <p><strong>2. VLAs are based on Markov Assumption, but World is not MDP</strong></p>
          <p> 
            pi0 is a <strong>memory-less</strong> policy, meaning its next action depends only on the current observation—not past actions. When tasks involve <strong>subtle temporal dependencies</strong>, this behavior can cause <strong>valid-looking but incomplete behavior</strong>.

          </p>
          <ul>
            <li><strong>Case</strong>: "Open the drawer" → stops after grasping the handle.</li>
            <li><strong>Behavior</strong>: Pi0 grasps the handle, then does nothing. The output actions are very small. </li>
          </ul>
          <p>
            VLA assumes task completion after the initial grasp, unaware that <strong>drawer articulation is a multi-step process</strong>. Pi0 always picks the most likely (mode) action. It is the failure you'd expect when the Markovian property does not hold for a given task. If the majority of training data in a given context (e.g. robot in home position, grasped drawer handle) corresponds to no motion, pi0 will always output "zero joint velocity"—and get stuck.
          </p>
          
          

        
          <div class="columns">
            <div class="column is-half">
              <p><strong>3. Token Decoding Edge Cases</strong></p>
              <p>
                During inference, pi0 will throw out this error: <code>Error decoding tokens: cannot reshape array of size 79 into shape (8)</code>
              </p>
              <p>
                According to our discussion on <a href="https://github.com/Physical-Intelligence/openpi/issues/373">Github Issue#373</a>, sometimes the policy decoded mis-shaped actions occasionally during inference. 
                In official implementation, <strong>pi0-fast-droid</strong> defaults to "no-motion" in these cases. Since as the robot continues querying the policy, 
                the error gets skipped on subsequent queries, allowing the robot to quickly recover and continue decoding correctly-shaped outputs.
              </p>
              <p><em style="color: red;">Warning: Don't kill PI0 Inference Server when early stops, be careful the robot may continue moving before server restarts!</em></p>
            </div>
            <div class="column is-half">
              <div>
                <img src="static/images/eval_Articulation_open the upper drawer_visualization.png" alt="Drawer articulation visualization" width="100%" style="border-radius: 10px;">
                <p class="is-size-6 has-text-centered">A visualization of Robot Joint and Gripper state of early stops</p>
              </div>
            </div>
          </div>
        
        </div> <!-- End of 4.1 -->





        <!-- Imprecise spatial reasoning -->
      <h3 class="title is-4">4.2 Imprecise spatial reasoning</h3>
        <div class="content has-text-justified">
          <p>
            Pi0 often struggles with spatial reasoning over height. For example, when asked to pick up an object and place it into a container, the policy cannot lift the object high enough to clear the height of the container. This suggests the drawback of <strong>vision-based policies</strong>: it does not have a metrically accurate method to know the distance between gripper and surrounding environment. 
          </p>

          <div class="columns">
            <div class="column is-half">
              <p>
              As shown here, the robot seems to think the gripper is high enough, and so it pushes the destination container. If the policy are provided with <strong>Camera sensor</strong>, then it should be able to accurately estimate the size of the object that it holds, and that of the gap between the height of the object and the bowl, and understand that it must increase the gap, it could complete the task successfully.
              </p>
            </div>

            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/Pushing_Bowl_Fail.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the pink bowl" - Pi0 fails to lift high enough and always collide</p>
              </div>
            </div>
        </div>

        <div class="columns">
          <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.2_touch_the_index_finger_of_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Touch the index finger of outstretched hand" - Pi0 correctly touch, but with gripper force</p>
              </div>
            </div>
            <div class="column is-half">
              <p>
                We also tried to prompt pi0 to raise the gripper higher. Like "raise the bottle high enough / up 10cm to avoid collision...". But it didn't learn on them.
                When pi0 is asked to operate with an articulation object, it becomes even harder for it to estimate the distance from the side view camera, causing frequent collisions. This is particularly worth noting when the robot is intersecting with humans. As it doesn't have any safety constraint,  it will accidentally hit / grasp the user's hand, which can be hurt!. 
              </p>
            </div>
          </div>

          <p>
            What's more, when Pi0 is required to manipulate a new household appliance, it will tend to collide with the device or stop during inference trials. As shown below, pi0 can not use the Coffee Machine in our lab.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_take_out_the_cup_under_the_coffee_machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Take out the cup under the coffee machine" → collides with top of machine</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_raise the lever of coffee machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Raise the lever of coffee machine" → didn't understand where is lever.</p>
              </div>
            </div>
          </div>

          <p>
            One possible solution is to involve concepts like Vox map and planning constraints. Depth cameras would also be very helpful to implement collision avoidance.
          </p>
        </div>

      <h3 class="title is-4">4.3 Sensitivity to Distribution Shift</h3>
        <div class="content has-text-justified">
          <p>
            It is well-known that most robot policies are sensitive to distribution shifts. Change the camera slightly or the initial position of the objects and the success rate can plummet. We found that Pi0 displays some robustness to perturbations, but performance can be dramatically improved by moving the test distribution closer to the training dataset.
          </p>

          <h5 class="title is-6">1. Orientation of objects significantly affects performance</h5>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_place_the_can_into_red_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the can into the red tray" - Success rate varies with can orientation</p>
              </div>
            </div>
            <div class="column is-half">
              <p>
                As shown, when the spam can is parallel to the viewer, the robot fails consistently. When perpendicular, it succeeds consistently. This orientation bias likely stems from the distribution of demonstrations in the training data, where certain viewpoints were overrepresented.
              </p>
            </div>
          </div>

          <h5 class="title is-6">2. Grasps at awkward spots on some objects</h5>
          
          <p>
            There is no built-in, geometry-aware grasp detection system. This highlights fundamental limitations:
          </p>

          <ul>
            <li><strong>Demonstration Bias:</strong> The policy shows clear preferences for grasping objects in ways that mirror the training demonstrations, even when those aren't optimal for the current scenario. For example, we observed pi0 attempts to pick up cups from their edges rather than their handles.</li>
            <li><strong>Distribution Mismatch:</strong> When objects appear in positions or orientations underrepresented in the training data, performance drops dramatically. This covariate shift is a well-known weakness of behavior cloning - the policy hasn't learned to recover from states outside its training distribution.</li>
          </ul>

          <p>
            The model doesn't actually know where & how to manipulate objects semantically - it can only reproduce patterns from demonstrations without true understanding of object affordances or functional parts. Though scaling up data may solve some scenarios, there are countless edge cases that need to be labeled.
          </p>

          <p>
            Potential solutions might involve augmenting behavior cloning with:
          </p>

          <ul>
            <li>Explicit object-centric representations to identify functional parts</li>
            <li>Adversarial training to improve robustness to distribution shifts</li>
            <li>Incorporating physical priors about stable grasps and object affordances</li>
          </ul>
        </div>
    </div>
    
</section>


<!-- Quirks Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">5. Quirk: Some interesting behavior of pi0</h2>

        <h4 class="title is-5">Quirk 1: Prompt Engineering Matters</h4>
        <div class="content has-text-justified">
          <p>
            Pi0's performance heavily depends on language specificity and prompt engineering. Let's look at two interesting aspects:
          </p>

          <h5 class="title is-6">1.1 You need to tune your prompt carefully to operate the robot</h5>
          
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Instruction</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>"Close the toilet"</em></td>
                <td>0%</td>
                <td>Wanders aimlessly, unable to localize the target.</td>
              </tr>
              <tr>
                <td><em>"Close the white lid of the toilet"</em></td>
                <td>100%</td>
                <td>Precise alignment and execution.</td>
              </tr>
            </tbody>
          </table>
          
          <p><strong>Why This Happens</strong>:</p>
          
          <ul>
            <li>Vague instructions (e.g., <em>"toilet"</em>) force Pi0 to guess the target sub-component.</li>
            <li>Specificity (e.g., <em>"white lid"</em>) aligns with PaliGemma's object-part grounding capability.</li>
            <li>Pi0 freezes or fails when instructions contain typos, grammatical errors, or ambiguous phrasing.</li>
          </ul>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\close_the_white_lid_of_the_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the white lid for the toilet (Success)</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\failure\close_the_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the toilet (Failure)</p>
              </div>
            </div>
          </div>

          <h5 class="title is-6">1.2 Pi0 Behavior Without Language Goals</h5>
          
          <p>
            When given no specific language instruction, Pi0 defaults to interacting with the most familiar objects from its training data:
          </p>

          <ul>
            <li>Given nonsense text like <em>"dgbfzjkfhjilawhdfkAWHDKLWHADFiQAWFHqawipfjcasklfmdc"</em>, it picks up marker pens</li>
            <li>Given <em>"xxx"</em>, it reaches for blocks repeatedly</li>
          </ul>

          <p>
            This behavior stems from dataset biases - for example, marker pens comprise 16.67% of objects in the DROID dataset, making them a frequent default choice.
          </p>

          <div class="box">
            <h5 class="title is-6">Key Implications</h5>
            <p>These language quirks highlight important areas for improvement:</p>
            <ul>
              <li><strong>Language Model Limitations:</strong> Pi0's 3B language model lacks the robustness of larger LLMs</li>
              <li><strong>Dataset Dependencies:</strong> Default behaviors are heavily influenced by training data distribution</li>
              <li><strong>Prompt Engineering:</strong> Success often requires careful instruction crafting</li>
            </ul>
          </div>
        </div>


        <h4 class="title is-5">Quirk 2: Visual Robustness</h4>
        <div class="content has-text-justified">
          <p>
            One of the most frequently asked questions on Pi0 is, how robust it is when the sensory inputs are disrupted? We did several tests on blocking the camera and the object. Here's what we found.
          </p>

          <h5 class="title is-6">Camera Blocking Experiments</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the pink object and place it into the bowl."</li>
            <li><strong>Cameras</strong>: Side-view (primary) + wrist-mounted (secondary).</li>
            <li><strong>Blocking Scenarios</strong>: Partial/full occlusion of one or both cameras.</li>
          </ul>

          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Blocking Type</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>No Block</strong></td>
                <td>100%</td>
                <td>Flawless execution. Picks pink object, attempts secondary objects.</td>
              </tr>
              <tr>
                <td><strong>Block Side Camera Mid-Trial</strong></td>
                <td>50%</td>
                <td>Relies on wrist cam. Gripper misalignment → partial success.</td>
              </tr>
              <tr>
                <td><strong>Block Wrist Camera Entirely</strong></td>
                <td>0%</td>
                <td>Frozen—no recovery.</td>
              </tr>
              <tr>
                <td><strong>Block Both Cameras Initially → Unblock Mid-Trial</strong></td>
                <td>70%</td>
                <td>Chaotic exploration → stabilizes after unblocking.</td>
              </tr>
            </tbody>
          </table>

          <h5 class="title is-6">Key Observations</h5>
          <ul>
            <li><strong>Active Perception</strong>:
              <ul>
                <li>When the side camera is blocked, Pi0 uses its wrist camera to retry grasps ("hand-eye coordination").</li>
                <li>Example: In Trial 3 (side camera blocked), Pi0 adjusted its gripper position twice to finally secure the pink object.</li>
              </ul>
            </li>
            <li><strong>Viewpoint Robustness</strong>:
              <ul>
                <li>Pi0 tolerates shifted camera angles mid-task.</li>
                <li>Blocking then unblocking cameras triggered exploratory movements (e.g., sweeping the arm to re-localize objects).</li>
              </ul>
            </li>
            <li><strong>Failure Modes</strong>:
              <ul>
                <li>Total wrist camera blockage → robot freezes (no fallback perception).</li>
                <li>Over-reliance on initial frames: If objects shift after blocking, Pi0 struggles to update its spatial model.</li>
              </ul>
            </li>
          </ul>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\close_the_drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"> "Close the drawer"</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\close_the_right_cabinet_door.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"> "Close the right cabinet door"</p>
              </div>
            </div>
          </div>

          <h5 class="title is-6">Object Blocking Experiment</h5>
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the red box."</li>
            <li><strong>Occlusion Levels</strong>: None (fully visible), 50% occluded, 100% occluded.</li>
            <li><strong>Behavior Metrics</strong>: Success rate, recovery attempts, and failure modes.</li>
          </ul>

          <p>
            We sought to determine how the robot would respond when the target of its task was occluded. When the box was fully visible, the robot would succeed. When it was 50% occluded, the robot would generally succeed, sometimes moving the occluding object to get a better view or grasp of the target. When fully occluded, the robot would become confused and stop. During one fully occluded trial, the robot knocked over the occluder and the red box, revealing more information about the location of the target but making it impossible to proceed with a good grasp.
          </p>
        </div>


        <h4 class="title is-5">Quirk 3: Active Perception & Viewpoint Robustness</h4>
        <div class="content has-text-justified">
          <p>
            One of the most frequently asked questions on Pi0 is, how robust it is when the sensory inputs are disrupted? We did several tests about blocking the camera, the object and moving the side view camera. Here's what we learned.
          </p>
          
          <h5 class="title is-6">Camera Blocking Experiments</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the pink object and place it into the bowl."</li>
            <li><strong>Cameras</strong>: Side-view (primary) + wrist-mounted (secondary).</li>
            <li><strong>Blocking Scenarios</strong>: Partial/full occlusion of one or both cameras.</li>
          </ul>
          
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Blocking Type</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>No Block</strong></td>
                <td>100%</td>
                <td>Flawless execution. Picks pink object, attempts secondary objects.</td>
              </tr>
              <tr>
                <td><strong>Block Side Camera Mid-Trial</strong></td>
                <td>50%</td>
                <td>Relies on wrist cam. Gripper misalignment → partial success.</td>
              </tr>
              <tr>
                <td><strong>Block Wrist Camera Entirely</strong></td>
                <td>0%</td>
                <td>Frozen—no recovery.</td>
              </tr>
              <tr>
                <td><strong>Block Both Cameras Initially → Unblock Mid-Trial</strong></td>
                <td>70%</td>
                <td>Chaotic exploration → stabilizes after unblocking.</td>
              </tr>
            </tbody>
          </table>
          
          <h5 class="title is-6">Key Observations</h5>
          
          <ol>
            <li>
              <strong>Active Perception</strong>:
              <ul>
                <li>When the side camera is blocked, Pi0 uses its wrist camera to retry grasps ("hand-eye coordination").</li>
                <li>Example: In Trial 3 (side camera blocked), Pi0 adjusted its gripper position twice to finally secure the pink object.</li>
              </ul>
            </li>
            <li>
              <strong>Viewpoint Robustness</strong>:
              <ul>
                <li>Pi0 tolerates shifted camera angles mid-task.</li>
                <li>Blocking then unblocking cameras triggered exploratory movements (e.g., sweeping the arm to re-localize objects).</li>
              </ul>
            </li>
            <li>
              <strong>Failure Modes</strong>:
              <ul>
                <li>Total wrist camera blockage → robot freezes (no fallback perception).</li>
                <li>Over-reliance on initial frames: If objects shift after blocking, Pi0 struggles to update its spatial model.</li>
              </ul>
            </li>
          </ol>
          
          <h5 class="title is-6">Object Blocking Experiment</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the red box."</li>
            <li><strong>Occlusion Levels</strong>: None (fully visible), 50% occluded, 100% occluded.</li>
            <li><strong>Behavior Metrics</strong>: Success rate, recovery attempts, and failure modes.</li>
          </ul>
          
          <p>
            We sought to determine how the robot would respond when the target of its task was occluded. In order to do this, we asked it to grasp a box, varying the occlusion level from fully visible, to 50%, to 100% occluded. When the box was fully visible, the robot would succeed. When it was 50% occluded, the robot would generally succeed, sometimes moving the occluding object in order to get a better view or grasp of the target. When it was fully occluded, though, the robot would become confused and stop. During one fully occluded trial, the robot knocked over the occluder and the red box, revealing more information about the location of the target but making it impossible for the robot to proceed with a good grasp.
          </p>
          
          <h5 class="title is-6">Why This Matters</h5>
          
          <p>
            Pi0's ability to <strong>improvise with partial observations</strong> hints at emergent active perception—a critical feature for real-world deployment where lighting, occlusions, or camera failures are inevitable.
          </p>
          
          <p><em>Left: Blocking the side camera forces Pi0 to rely on wrist-view. Right: Success rates across blocking scenarios.</em></p>
          
          <p>
            This quirk underscores Pi0's potential as a resilient generalist—though it's no substitute for dedicated SLAM or depth sensing... yet.
          </p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/camera_blocking_experiment.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Camera blocking experiment</p>
        </div>
      </div>
    </div>
  </div>
        

        
      </div>
    </div>
  </div>
</section>


<!-- Future Directions -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">6. The Bigger Picture</h2>

          <div class="content">
          <h3 class="title is-4">6.1 Why Pi0 Matters</h3>
          
          <div class="columns">
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Generalization</h4>
                <p>Pi0 shows decent zero-shot performance across a wide range of tasks. For centuries, humans have had a strong desire to build a generalist agent system that could work in the wild, going out of the robotics lab and assisting our daily life. With the rapid development of VLAs model, the existence of Pi0 marks a milestone for robotics research.</p>
          </div>
        </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Scalability</h4>
                <p>FAST tokenizer enables 5x faster training, which is an impressive milestone and turning point for VLA research. The effective utilization of data implies that what matters to robots is not only the scaling size of dataset, but also the representation's expressiveness.</p>
              </div>
            </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Open Source</h4>
                <p>The open-sourcing of Pi0 will be very useful and inspiring for the whole robotics industry. The impact not only is that every lab gets a very powerful manipulation baseline, but it will also push the horizon robotics manipulation capabilities with a large scale framework that somehow work in the wild. 
                  
                  <!-- As stated in this  <a href="https://arxiv.org/abs/2409.09491">paper</a>, a fair comparison, beyond simple success rate, may benefit the problem scoping, allowing robotics research to grow as fast as computer vision and general artificial intelligence (<a href="https://arxiv.org/abs/2409.09491">Kress-Gazit etc, 2024</a>).
                   -->
                </p>
              </div>
            </div>
          </div>
          
          <h3 class="title is-4">6.2 Future Directions</h3>
          
          <!-- <div class="columns">
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Better Language Grounding</h4>
                <p>As discussed in PI's new work "Hi Robot", VLA's language understanding ability could be enhanced via a two-tiered inference framework. However, as we saw in our use cases, the 3B VLM backbone may still suffer from instruction following & generalization across language axes.</p>
              </div>
            </div> -->
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Force Feedback</h4>
                <p> Current models rely primarily on visual input. Incorporating force feedback could help with tasks requiring delicate pressure control and improve interaction with varied surfaces. For example, to plug in a coffee handle into an espresso machine requires a sense of force direction, it may not be solvable without force sensing. </p>
              </div>
            </div>


            <div class="column">
              <div class="content">
                <h4 class="title is-5">Active Perception</h4>
                <p>Although pi0 exhibits certain active perception ability, its searching ability is still limited to a short range of wrist camera. Adding a module to help it explore the surrounding environment could be a useful direction to augment existing generalist policies.</p>
              </div>
            </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Memory Modules</h4>
                <p>Currently, Pi0's memory-less system has achieved great success, but lacking history/memory is a significant limitation. Future models will likely incorporate explicit memory mechanisms to maintain task state across interruptions.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">7. Conclusion</h2>

        <div class="content has-text-justified">
          <p>Pi0 represents a promising step toward generalist robots policies, but  challenges remain. <strong>Instruction following and fine-grained tasks are still very challenging </strong> As foundation models evolve, we're optimistic—soon, every robotics lab might have a Pi0-like baseline, and we will get some strong VLA for real-world generalization!</p>
          
          <div class="buttons is-centered mt-5">
            <a href="#" class="button is-link is-medium">
              <span class="icon">
                <i class="fas fa-table"></i>
              </span>
              <span>Explore Full Results</span>
            </a>
            
            <a href="#" class="button is-success is-medium">
              <span class="icon">
                <i class="fas fa-video"></i>
              </span>
              <span>Watch Full Videos</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- References -->
<section class="section" id="references">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">8. References</h2>
        <div class="content">
          Place holder for more papers TODO fix
          <div class="csl-bib-body">
            <div class="csl-entry">
              Driess, D., Xia, Z., Ha, L., Kaplan, A., Huang, H., Hausman, K., Ichter, B., Fox, D., & Levine, S. (2024). 
              <span class="paper-title">Pi0: Prompting in the Wild Enables Zero-Shot Robot Manipulation.</span>
              <span class="paper-conference">arXiv preprint arXiv:2401.08744.</span>
              <a href="https://arxiv.org/abs/2401.08744">[Paper]</a>
              <a href="https://pi-zero.github.io/">[Project Page]</a>
            </div>
            <div class="csl-entry">
              Xia, Z., Driess, D., Ha, L., Kaplan, A., Huang, H., Hausman, K., Ichter, B., Fox, D., & Levine, S. (2024).
              <span class="paper-title">DROID: Embodied Compositional Tasks Driven by Language Models.</span>
              <span class="paper-conference">arXiv preprint arXiv:2401.08745.</span>
              <a href="https://arxiv.org/abs/2401.08745">[Paper]</a>
              <a href="https://droid-dataset.github.io/">[Project Page]</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Acknowledgments -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">9. Acknowledgments</h2>
        <div class="content has-text-justified">
          <p>
            We thank Will Liang, Hungju Wang, Sam Wang for their help in setting up the pi0 environment. We also thank Kaustubh Sridhar, Tianyou Wang, Ian, Ethan Yu, Tim Song and Yiqian Li for their help in doing experiments.
          </p>
          <p>
            We thank Junyao Shi, Aurora Qian, Leon Kim and Jason Ma for the suggestions on how to evaluate a generalist manipulation policy.
          </p>
          <p>
            We are also grateful to Karl Pertsch from Physical Intelligence for comments on the blog draft.
          </p>
          <p>
            This work is independently conducted within PennPAL Group and Daniilidis Group under GRASP Lab.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Citation TODO change site-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <p>If you find this evaluation useful for your research, please consider citing our repository:</p>
          <pre><code>@misc{pi0-experiment-wild,
  author = {PAL Research Group},
  title = {Pi0 Experiment in the Wild},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/penn-pal-lab/Pi0-Experiment-in-the-Wild}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Conclusion Image -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <figure class="image">
            <img src="static/images/robot_workspace.jpg" alt="Robot Workspace">
            <figcaption>Our robot workspace setup used for evaluating Pi0.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Robot & Model Setup Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Appendix A: Our Robot & Model Setup</h2>

        <div class="content has-text-justified">
          <p>The following are details of our experiment set up.</p>
        </div>
        
        <h3 class="title is-4">3.1 Hardware:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Franka Research 3 Arm</strong>: 7-DOF force-sensitive robot with a 3 kg payload.</li>
            <li><strong>Robotiq 2F-85 gripper:</strong> two-finger gripper with 5mm stroke and adjustable force control.</li>
            <li><strong>Cameras</strong>:
              <ul>
                <li><strong>Side-view:</strong> ZED 2 stereo camera for global scene understanding</li>
                <li><strong>Wrist-mounted:</strong> ZED Mini for close-range object manipulation</li>
                <li><strong>Perception Mode:</strong> <strong>Pure RGB</strong> (no depth calibration)</li>
              </ul>
            </li>
          </ul>
          
          <figure class="image">
            <img src="static/images/robot.png" alt="Robot setup">
          </figure>
      </div>
        
        <h3 class="title is-4">3.2 Computing</h3>
        <div class="content has-text-justified">
          <p><strong>GPU Server:</strong></p>
          <ul>
            <li><strong>GPUs:</strong> 1x NVIDIA RTX A6000 (48GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.3</li>
            <li><strong>Usage:</strong> VLA model inference.</li>
          </ul>
          
          <p><strong>Workstation</strong></p>
          <ul>
            <li><strong>GPU:</strong> NVIDIA GeForce RTX 3080 (16GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.6</li>
            <li><strong>Usage:</strong> DROID low level control.</li>
          </ul>
        </div>
        
        <h3 class="title is-4">3.3 Pi0-FAST-DROID:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Vision-Language Model</strong>: <em>Paligemma 3B</em> for spatial and semantic understanding.</li>
            <li><strong>FAST+</strong>: Frequency-space Action Sequence Tokenization (FAST), a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies.
            <li><strong>Training Data</strong>: Pretrained on π cross-embodiment robot dataset & Open X-Embodiment, fine tuned on DROID dataset.</li>
          </ul>
          
          <figure class="image">
            <img src="static/images/dataset.jpg" alt="Large scale robotics data for pretraining">
            <figcaption>Dataset used for pretraining pi0</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Appendix B: Detailed Results for Each Task</h2>

        <!-- Overall Results -->
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="box">
              <h3 class="title is-4">Overall Performance</h3>
              <div class="columns">
                <div class="column is-half">
                  <figure class="image">
                    <div class="columns">
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-scatter.png" alt="Scatter plot showing performance distribution">
                      </div>
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-bar.png" alt="Chart showing overall performance statistics">
                      </div>
                    </div>
                    <figcaption>Performance across 240+ trials (52% average progress)</figcaption>
                  </figure>
                </div>
                <div class="column is-half">
          <div class="content">
                    <p>Across our 240+ test trials, Pi0 achieved varying degrees of success:</p>
                    <ul>
                      <li><strong>Complete Success:</strong> 38%</li>
                      <li><strong>Partial Success:</strong> 28%</li>
                      <li><strong>Complete Failure:</strong> 34%</li>
            </ul>
                    <p>We observed that performance varied significantly based on task type, environmental conditions, and most importantly, the phrasing of instructions.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
        </div>
        
        <!-- Task Specific Results -->
        <h3 class="title is-4">Task-Specific Performance</h3>
        
        <!-- Pick and Place -->
        <div class="box">
          <h4 class="title is-5">Pick-and-Place (44% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Familiar objects (e.g., pineapple toy, markers) - 90% success</li>
                  <li>Clear spatial targets ("in the pink bowl") - 85% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Large objects (black cube) - 35% success</li>
                  <li>Vague targets ("inside the bowl") - 40% success</li>
                  <li>Multi-object tasks ("Put all objects into the basket") - 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <figure class="image">
                <img src="static/images/pick_and_place_chart.jpg" alt="Chart showing pick and place performance">
              </figure>
            </div>
          </div>
        </div>
        
        <!-- Human Interaction -->
        <div class="box">
          <h4 class="title is-5">Human Interaction (53.5% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Object handovers - 46.67% success</li>
                  <li>Following a moving human - 65% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Precision interactions ("Shake hands") - 30% success</li>
                  <li>Recovery after interruption - 40% success</li>
                </ul>
              </div>
            </div>
      
            <div class="column is-4">
              <div class="columns is-multiline">
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\success\human_3_Pick up the pineapple and give it to the programmer.mp4" type="video/mp4">
      </video>
                  <p class="is-size-6 has-text-centered">"Give the pineapple to the programmer"</p>
    </div>
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\failure\human_7_Pick up the whiteboard eraser and give it to the programmer.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Give the whiteboard eraser to the programmer"</p>
  </div>
              </div>
            </div>

          </div>
        </div>
        

        <!-- Coffee Machine -->
        <div class="box">
          <h4 class="title is-5">Coffee Machine (8.00% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Capsule Coffee Machine:</strong></p>
                <ul>
                  <li>Close the capsule lid of coffee machine - 50% success</li>
                  <li>Pick up the capsule from the coffee machine - 0% success</li>
                  <li>Place the capsule into the coffee machine - 0% success</li>

                </ul>
                <p><strong>Espresso Coffee Machine:</strong></p>
                <ul>
                  <li>Pick up the coffee portafilter - 0% success</li>
                  <li>Pour the coffee into the cup - 0% success</li>
                  <li>Pick up the silver milk frothing pitcher- 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <div class="columns">
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/failure/coffee_failure.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Place the capsule into the coffee machine"</p>
                </div>
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/right_Close_the_capsule_lid_of_the_coffee_machine.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Close the capsule lid of the coffee machine"</p>
                </div>
              </div>
            </div>
          </div>
        </div>
        
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>Pi0 Evaluation</strong> by <a href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">PAL Research Group</a>, GRASP Lab, University of Pennsylvania.
      </p>
      
      <p>
        The website template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
</footer>

</body>
</html>
