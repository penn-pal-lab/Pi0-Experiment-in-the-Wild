<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating \( \pi_0 \) in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies">
  <meta name="keywords" content="pi0, Robotics, VLA, Vision-Language-Action, PennPAL, Kostas Daniilidis, GRASP Lab">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Evaluating \( \pi_0 \) in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax (LaTeX→HTML) -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://grasp.upenn.edu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://grasp.upenn.edu/">
            GRASP Lab
          </a>
          <a class="navbar-item" href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">
            PennPAL Research Group
          </a>
          <a class="navbar-item" href="https://www.cis.upenn.edu/~kostas/">
            Daniilidis Research Group
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
<section class="hero">
  <!-- <div class="hero-body"> -->
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Evaluating \( \pi_0 \) in the Wild:<br>Strengths, Problems, and the Future of Generalist Robot Policies</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://everloom-129.github.io/">Jie Wang</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.grasp.upenn.edu/people/matthew-leonard/">Matthew Leonard</a>,</span>
            <span class="author-block">
              <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,</span>
            <span class="author-block">
              <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>,</span>
            <span class="author-block">
              <a href="https://edwardshu.com/">Edward S. Hu</a> 
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">GRASP Lab, University of Pennsylvania</span>
          </div>
          <div class="is-size-10 publication-authors">
            <span class="author-block">*Corresponding author</span> 
          </div>
        </div>
      </div>
    </div>
  <!-- </div> -->
</section>


<!-- Citation Box -->
<section class="section" style="padding-top:1rem; padding-bottom:1rem;">
  <div class="container is-max-desktop">
    <div class="box has-background-light is-size-7">
      <p class="has-text-weight-semibold mb-1">Please Cite&nbsp;this&nbsp;work if you find it helpful!</p>
      <pre style="white-space:pre-wrap;font-size:0.65rem;line-height:1.2em;background:transparent;border:0;padding:0;">J.&nbsp;Wang*, M.&nbsp;Leonard, K.&nbsp;Daniilidis, D.&nbsp;Jayaraman, &amp; E.&nbsp;S.&nbsp;Hu. (2025). <em>Evaluating \( \pi_0 \) in the Wild: Strengths, Problems, and the Future of Generalist Robot Policies.</em><a href="https://penn-pal-lab.github.io/pi0-Experiment-in-the-Wild" target="_blank" rel="noopener">Blog&nbsp;</a></pre>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="static/videos/success/success_novel_fold_newspaper.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      pi0 commanded to fold a newspaper, having never seen this particular workspace before.
      </h2>
    </div>
  </div>
</section>
<script>
  const video = document.getElementById('teaser');
  video.playbackRate = 3.0;
</script> -->


<!-- TL;DR Box -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="box has-background-light">
          <h3 class="title is-4 has-text-centered">TL;DR</h3>
          <div class="content">
            <ul class="is-size-5">
              <li><strong>Overall Progress:</strong> 42% average progress score towards completion across 300+ diverse trials</li>
              <li><strong>Key Finding:</strong> \( \pi_0 \) demonstrates impressive vision-language understanding, but it struggles with spatial reasoning and precise manipulation</li>
              <li><strong>Bottom Line:</strong> \( \pi_0 \) can perform well in the wild, but its success depends on the details of the prompt, and how well the task matches the training distribution.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Key Findings Section -->
<section class="section" style="padding-top: 1rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-3">2. Key Findings</h2> -->

        <!-- Hero Images -->
        <!-- <div class="columns is-centered">
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-scatter.png" alt="Scatter plot showing performance distribution">
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-bar.png" alt="Bar plot showing performance distribution">
            </figure>
          </div>
        </div> -->
        <!-- <p class="has-text-centered" style="margin-top: 0rem;">
          <strong>Left</strong>: relationship between score and number of trials; <strong>Right</strong> the progress score on each task.<br> <br>
        </p> -->
        <!-- Hero Images End -->
        <div class="content has-text-justified">
          Robotics, particularly manipulation, has never had learned models that work out of the box on new objects, locations, and tasks. Roboticists have had the unsatisfying experience of going through tedious engineering and data collection to acquire robot policies, and then finding that even small environmental changes break those policies.
          One promising direction is to train generalist models on large datasets, in the hope that they will produce sensible behavior in new situations, reducing the burden on the end user.
          This last year has been exciting because of the first wave of models that are beginning to promise that this dream of generalist robots is possible.
          So, when Physical Intelligence made their models public,  we were keen to try it out ourselves, and we were largely impressed and excited about the possibilities as these models continue to improve.
        </div>
        <div class="hero-body">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="static/videos/success/success_novel_fold_newspaper.mp4" type="video/mp4">
          </video>
          <p class="subtitle has-text-centered">
          pi0 commanded to fold a newspaper, having never seen this particular workspace before.
          </p>
        </div>


        <!-- 
        We evaluated the pi0 fast model.description of the workspace, and the evaluation setup.
         -->
        <div class="content has-text-justified">
          Our evaluations were conducted using the <strong>π₀-FAST-DROID</strong> model, specifically fine-tuned on <strong>DROID robot setup</strong>, which consists of the Franka Panda robot with side and wrist cameras. We found it refreshingly easy to setup the platform for policy inference  - no camera / controller calibration, or workspace-specific tuning was required. To output actions, the model requires a prompt from the user, describing the task, and images from the wrist and side cameras (see video above).
        </div>
              <div class="hero-body" style="padding: 1.5rem 1.5rem;">
                <div class="columns is-centered">
                  <div class="column">
                    <div class="content">
                      <!-- First row - three images side by side -->
                      <div class="columns is-vcentered is-gapless" style="margin-bottom: 1rem;">
                        <div class="column is-one-third">
                          <figure class="image" style="margin: 0;">
                            <img src="static/images/lab_setup.jpg" alt="GRASP Lab Setup" style="object-fit: cover; height: 200px; width: 100%;">
                          </figure>
                      </div>
                        <div class="column is-one-third">
                          <figure class="image" style="margin: 0;">
                            <img src="static/images/robot_workspace.jpg" alt="Robot Workspace" style="object-fit: cover; height: 200px; width: 100%;">
                          </figure>
                      </div>
                        <div class="column is-one-third">
                          <figure class="image" style="margin: 0;">
                            <img src="static/images/lab_setup_2.jpg" alt="Levine Setup 2" style="object-fit: cover; height: 200px; width: 100%;">
                          </figure>
                        </div>
                      </div>
                      
                      <!-- Second row - manipulation scene (full width) -->
                      <div class="columns is-centered" style="margin-top: 0.5rem;">
                        <div class="column is-12">
                          <figure class="image">
                            <img src="static/images/wild-newsppaper.png" alt="Three Camera View" style="width: 100%;">
                          </figure>
                        </div>
                      </div>
                    
                      <p class="has-text-centered" style="margin-top: 1rem;">
                        Our evaluation setup showing the robot workspace, test objects, and typical manipulation scenes.
                      </p>
                    </div>
                  </div>
                </div>
              </div>
        <div class="content has-text-justified">
          <p>
            Our experiments took place in <a href="https://facilities.upenn.edu/maps/locations/levine-hall-melvin-and-claire-weiss-tech-house">Levine Hall 457</a> at Penn in a mock kitchen environment, see images above. The environment has a diverse selection of objects, backgrounds and lighting conditions, making it ideal for coming up with a large variety of tasks.
            <!-- talk about the vibe-based evaluation here. -->
            <!-- Inspired by how how users rank LLMs by chatting about arbitrary topics in Chatbot Arena, we subject pi0 to "vibe checks," which are unstructured real world experiments. Users improvise tasks, alter camera angles, rearrange objects, and try to think of edge cases where they expect the policy might fail. We want to know whether pi0's training data distribution is wide enough to cover the range of tasks that humans can come up with. So, we test the policy's generalization capabilities, and discover some interesting phenomena that structured evaluations may not have found.
            Critically, vibe-based evaluation only becomes trustworthy when people can easily try out and verify the models themselves. Like open-source LLMs, Pi0's ease of deployment makes these kinds of evaluations accessible for any robotics lab with a DROID setup. We summarize our discoveries over 300+ trials of pi0 below, exploring its capabilities, quirks, and its implications for the future of robot learning. We hope that our work inspires others to try out pi0 for themselves! -->
            Evaluating robot policies is difficult because it is hard to come up with a selection of tasks that cover the wide range of behaviors an arbitrary user would find useful. 
          </p>
          <div class="columns"> 
            <div class="column is-half"> 
              We take inspiration from the NLP community, by adopting their "vibe checks" approach. Vibe-checking involves the user directly evaluating the LLMs themselves by chatting about whatever topic comes into mind, rather than relying on a standard benchmark. Similarly, we subject pi0 to "vibe checks," which are unstructured real world tasks generated by users. Our users improvise tasks, alter camera angles, rearrange objects, and try to think of edge cases to stress-test the model. By letting our users come up with whatever tasks that come into mind, we can test the policy's generalization capabilities, and discover some interesting phenomena that structured evaluations may not have found.
            </div>

            <div class="column is-half">  
              <img src="static/images/worldcloud.png" alt="Word cloud visualization" style="width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto;">
              <figcaption style="text-align: center; margin-top: 0.5em;">Word cloud of task prompts from our users.</figcaption>
            </div> 
          </div>


          <p>
          We conducted over 300 trials of pi0, and for the impatient reader, summarize our findings below:
          </p>

          <ol>
            <li>
              <strong>Strong Prior for Sensible Behaviors</strong>: pi0 produces sensible behaviors across a wide variety of our tasks, although it is important to note that sensible behaviors are often insufficient for task completion.
            </li>
            <li>
              <strong>Prompt Engineering Matters</strong>: While \( \pi_0 \) can produce sensible actions from a wide variety of prompt and camera views, its performance varies <strong>20~100%</strong> depending on prompt phrasing and camera viewpoint. Therefore to maximize performance, the policy inputs should be carefully designed for the exact task.
            </li>
            <li>
              <strong>Unexpected Quirks</strong>: \( \pi_0 \) can recover from failures, and handle moving humans in the scene, but it struggles with <strong>mid-task freezing, collision avoidance, and fine-grained manipulation</strong>
            </li>
          </ol>
          
          <p>
            Below, we elaborate on our findings in detail, going through success and failure behavior of pi0, as well as some additional interesting phenomena we discovered.
          </p>

          <!-- <h3 class="title is-4">Evaluation: Progress Score through Vibe Testing</h3> -->
          
          <!-- <p>
            Evaluating large generalist robot policies like <strong>pi0</strong> remains an open challenge. Unlike classical robotics benchmarks, these models operate across diverse tasks and open environments, where it is difficult to quantify a notion of success.
          </p> -->

          <!-- <p>
            In our blog, we adopt a <strong>progress score</strong> as the main evaluation signal, which is <strong>loosely inspired by the "vibe testing" rubric</strong> proposed in <a href="https://arxiv.org/abs/2403.04132">ChatbotArena(Chiang, etc, 2024)</a>, (<a href="https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation">Lambert 2024</a>). This method has also been used in recent works such as pi0 and pi0-FAST. During evaluation, we assign a continuous score from 0 to 100 to each policy rollout, reflecting the <strong>fraction of the commanded task successfully completed</strong>. For instance, for a <strong>newspaper-folding</strong> task, we might give the following scores:
          </p> -->

   
          <!-- <p>
            This scalar scoring allows us to assess success modes, failure modes, and emerging capabilities across the experiments we performed. As evaluation methods for VLAs develop further, we believe more standardized, benchmark-driven evaluation methods for generalist policies will emerge. For now, <strong>vibe-informed progress scores</strong> are a reasonable and transparent compromise.
          </p> -->

          <!-- <p>
            For more details, see <a href="#appendix">Appendix B: Results</a>.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Success Video -->
<section class="section" style="padding-top: 1rem;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Success Cases</h2>

        <!-- Success videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Pick and Place</h3>
                  <div>
                    <video id="success-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_pickplace_fish_to_box.mp4" type="video/mp4">
      </video>
    </div>
                  <p class="is-size-6">"Place the yellow fish into the purple box"</p>
                  <p class="is-size-6">Precise placement of the camouflage fish into the box</p>
  </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Articulation</h3>
                  <div>
                    <video id="success-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_articulation_open_drawer.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Open the drawer"</p>
                  <p class="is-size-6">Opens drawer with multiple pulls to make sure it is fully open</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Human Robot Interaction</h3>
                  <div>
                    <video id="success-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_hri_handover_pineapple.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Hand the pineapple to the programmer"</p>
                  <p class="is-size-6">Safe object handover to the programmer, even with wire occlusion from the side view</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Dexterity</h3>
                  <div>
                    <video id="success-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_dexterity_pour_water.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pour water from the silver cup to the pink bowl"</p>
                  <p class="is-size-6">Pours real water from the Latte art vat into the target bowl</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Multi-step Task</h3>
                  <div>
                    <video id="success-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_multistep_fill_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pick up all the objects into the basket"</p>
                  <p class="is-size-6">Sequential placement of all the toys into the basket</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Novel Objects</h3>
                  <div>
                    <video id="success-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/right_Close_the_capsule_lid_of_the_coffee_machine_left.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Close the capsule lid of the coffee machine"</p>
                  <p class="is-size-6">Handles previously unseen device</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Remarkable Performance Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">3. Remarkable Performance of pi0</h2>

        <!-- Vision-Language Understanding -->
        <h3 class="title is-4">3.1 Robust Vision-Language Understanding in Complex Scenes</h3>
        <div class="content has-text-justified">
          <p>
            Powered by <strong>PaliGemma</strong> (Google DeepMind's 3B VLM) as its vision encoder (<a href="https://arxiv.org/abs/2407.07726">Beyer etc, 2024</a>), \( \pi_0 \) demonstrates robust scene comprehension and adaptability. Despite relying solely on uncalibrated monocular RGB inputs (224x224 pixels after compression), it can handle very challenging objects and environments, including transparent or camouflaged items, and items it has not seen during training.            
          </p>
          
          <p><strong>1. It can grasp transparent objects</strong></p>
          
          <p>
            \( \pi_0 \) is capable of identifying and manipulating transparent objects, as shown below. It picks up the bottle with a stable grasp, aligns it to the small cup, and precisely drops it in. Many traditional grasp detection techniques require an accurate 2D or 3D reconstruction of the scene, and transparent objects can cause issues in reconstruction accuracy. This makes it even more impressive that the model can detect transparent objects solely from uncalibrated, mono-RGB views from the wrist camera and one side view camera.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\Success_Dropping_Into_Cup.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the white cup."</p>
        </div>
      </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\transparent_bottle_place.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the bowl."</p>
        </div>
      </div>
          </div>

          <p><strong>2. It can grasp an object even when it is camouflaged into a colorful background</strong></p>
          
          <p>
            \( \pi_0 \) can identify the 'yellow fish' here even when it is placed on top of a colorful board game. This object has an unusual and difficult shape, and it blends in well with the background, but \( \pi_0 \) detects it well enough to grasp it up.
          </p>
          
    <div class="columns is-centered">
            <div class="column is-half"> 
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/success/camflage_yellow_fish.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the red box"</p>
          </div>
        </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_pickplace_fish_to_box.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the purple box"</p>
      </div>
    </div>
  </div>
          
          <p><strong>3. It is robust to human activity in the input</strong></p>
          
          <p>
            During evaluation, there were many times where the side-view camera captured humans moving around in the background. However, \( \pi_0 \) can always focus on its task, keeping the robotic arm's movements focused on object manipulation.
          </p>
          
          <p>
            We believe there are two reasons for pi0's robustness to human movement. First, the pre-trained VLM backbone of \( \pi_0 \) is trained on images involving humans (<a href="https://aclanthology.org/P18-1238/">Sharma et al., 2018</a>, <a href="https://aclanthology.org/2022.naacl-main.142/">Changpinyo et al., 2022a</a>, <a href="https://storage.googleapis.com/openimages/web/factsfigures_v7.html">Kuznetsova et al. 2020</a>), so humans are in-distribution. Next, as our occlusion experiments in <strong>Section 2.3.1</strong> show, the policy seems to prioritize the wrist camera's images during pick-and-place tasks, so distractors in the side-view camera seem to minimally affect the policy.
          </p>
          
          <p>
            Here are two side-view videos involving humans in the scene. Please refer to <strong>Appendix B.6</strong> for more experiments on <strong>human-robot interaction</strong>.
          </p>

          <p><em>(All videos featuring humans were uploaded with permission from the individuals involved.)</em></p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_initial_PI0_pineapple.mp4" type="video/mp4">
      </video>
                <p class="is-size-6 has-text-centered">"Pick the pineapple and place it into the basket"</p>
    </div>
  </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_hand_the_pineapple_to_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to the outstretched hand"</p>
              </div>
            </div>
          </div>
            

          <p>
            Many existing works in Computer Vision and Robotics focus on transparent object detection and manipulation. But the nice part here is that we have an end-to-end, data-driven system that does it, without any special logic or care for transparent objects.
          </p>
          <p class="box has-background-grey-lighter has-text-centered is-italic">
            "pi0's ability to handle transparency, clutter and distractors hints at a future where robots see the world as humans do—through semantics, not just pixels."
          </p>
          
        </div>
        
        <!-- Behavioral Structure: \( \pi_0 \) is an FSM -->
        <h3 class="title is-4">3.2 Behavioral Sequencing in pi0</h3>
        <div class="content has-text-justified">
          <p>
            Through our experiments, we observed that \( \pi_0 \) exhibits intuitive patterns in its behavior across a wide range of manipulation tasks. Although it is an autoregressive model without any explicit memory or state, \( \pi_0 \) often progresses through distinct behavioral phases such as:
          </p>
          
          <p style="text-align: center;">
            <strong>Search → Reach → Grasp → Transfer → Release → Reset → Idle</strong>
          </p>

          <p>
            What's remarkable is that this structure <strong>emerges naturally from the data</strong> rather than being explicitly modeled — suggesting that \( \pi_0 \) learns consistent <strong>task execution priors</strong> across environments. For instance, even when \( \pi_0 \) is unfamiliar with an object or task, it often <strong>proactively explores</strong> near affordance-rich areas, using its wrist camera to decide whether to grasp or not.
          </p>

          <p>
            In certain trials, we also observed <strong>reset-like behaviors</strong>: if \( \pi_0 \) perceives the task as complete (e.g., after placing an item into a bowl), it may <strong>return to its home configuration and stop</strong>. While this often indicates a well-formed task boundary, it can also lead to <strong>early stopping/freezing</strong>, especially in multi-object scenes — see <strong>Section 4.1</strong> for analysis of early stopping failure cases.
          </p>

          <p>
            <em>While this sequencing might suggest that \( \pi_0 \) has learned an internal understanding of the task, we caution against such framing. These patterns may reflect properties of the data distribution (e.g., Markovian, short-horizon tasks), rather than indicating the policy has acquired explicit task inference or memory.</em>
          </p>

          <!-- Existing video grid -->
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_remove_the_pink_bowl_from_the_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Remove the pink bowl from the tray"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_stack_the_wooden_blocks.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Stack the wooden blocks"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_fold_the_cloth_from_left_to_right.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold the cloth from left to right"</p>
              </div>
            </div>
          </div>
          
          <!-- <p class="box has-background-grey-lighter has-text-centered is-italic">
            "pi0's FSM pattern indicates it is a strong meta learning algorithm being able to generalize across environments with pretrained data."
          </p> -->
        </div>
        
        <!-- Failure Recovery -->
        <!-- <h3 class="title is-4">3.3 Failure recovery and generalization</h3>
        <div class="content has-text-justified">
          <p>
            One key difference between the \( \pi_0 \) and other open source VLA models are: it can work in the wild, without online data fine-tuning, without calibration, without much position tuning on the policy, we can let it run on our franka robot directly.
          </p>
          
          <p>
            More interestingly, the VLA behaves similar to diffusion policy, as shown below, it could recover from failures and retry tasks that initially failed.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\3.3place_the_pen_into_pen_holder.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the pen into pen holder"</p>
              </div>
              <div>
            </div>
     
          </div>
          </div>

          <div class="columns is-multiline is-centered">
            <div class="column is-full">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\3.3_fold_up_the_newspaper_from_right_side.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold up the newspaper from left side"</p>
              </div>
            </div>
          </div>
          


        </div> -->
      </div>

      
    </div>
  </div>
</section>

<!-- Failiure Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Failure Cases</h2>
        <p class="has-text-justified" style="color: #666; font-style: italic;">
          \( \pi_0 \) demonstrates very impressive robustness across different tasks, locations, and lighting conditions. However, we have also observed some failure cases:
        </p>
        <!-- Failure videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Objects</h3>
                  <div>
                    <video id="failure-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\4_failure_pour_the_water_from_teapot_into_bowl.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour water from teapot into bowl"</i></p>
                  <p class="is-size-6">Cannot manipulate a novel glass teapot (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Background</h3>
                  <div>
                    <video id="failure-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\upenn_pick_block.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pick the black box on the white box"</i></p>
                  <p class="is-size-6">Cannot handle unseen background well (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Task Misunderstanding</h3>
                  <div>
                    <video id="failure-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_fish_into_the_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"place the yellow fish into the basket"</i></p>
                  <p class="is-size-6">Picks up the wrong object in cluttered scenes (25% success rate)</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Spatial Reasoning</h3>
                  <div>
                    <video id="failure-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_can_into_the_tray.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Place the can into the tray"</i></p>
                  <p class="is-size-6">Misjudges object position relative to container (30% success rate)</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">General Articulation</h3>
                  <div>
                    <video id="failure-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\video_close_the_right_cabinet_door.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Close the right cabinet door"</i></p>
                  <p class="is-size-6">Fails to open toy kitchen cabinet on the table (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Coffee Making</h3>
                  <div>
                    <video id="failure-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\coffee_pour_coffee_bean_into_grinder.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour coffee bean into the grinder"</i></p>
                  <p class="is-size-6">Cannot work with espresso machine (0% success rate)</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Problems with \( \pi_0 \) Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">4. Problems with pi0</h2>

        <!-- Early stopping problem -->
        <h3 class="title is-4">4.1 Early stopping</h3>
        <div class="content has-text-justified">
          <p>
            One common failure case is that policy may <strong>freeze unexpectedly during execution</strong>, leaving tasks incomplete until a human intervenes. 
            This behavior comes from two related factors: semantic ambiguity and autoregressive action decoding limitations.
          </p>


          <!-- 3 video examples of early stopping -->
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_hand_the_pineapple_to_human.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to human"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_open the drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Open the drawer"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_pour coffee bean into the coffee grinder.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Pour coffee bean into the coffee grinder"</p>
              </div>
            </div>
          </div>


          
          <h4 class="title is-5">Possible Causes</h4>
          
          <p><strong>1. The VLA doesn't understand the instruction</strong></p>
          <p>
            \( \pi_0 \) lacks the commonsense reasoning that LLMs can use to recognize unfamiliar object categories. When it does not understand a command, it gets stuck. 
            In some experiments, we found that some objects / instructions are out of distribution (OOD), causing early stopping.
          </p>
          
          
          <p><strong>2. VLAs are based on Markov Assumption, but the world is not an MDP</strong></p>
          <p> 
            \( \pi_0 \) is a <strong>memory-less</strong> policy, meaning its next action depends only on the current observation—not past actions. When tasks involve <strong>subtle temporal dependencies</strong>, this behavior can cause <strong>valid-looking but incomplete behavior</strong>.

          </p>
          <ul>
            <li><strong>Case</strong>: "Open the drawer" → stops after grasping the handle.</li>
            <li><strong>Behavior</strong>: \( \pi_0 \) grasps the handle, then does nothing. The output actions are very small. </li>
          </ul>
          <p>
            VLA assumes task completion after the initial grasp, unaware that <strong>drawer articulation is a multi-step process</strong>. \( \pi_0 \) always picks the most likely (mode) action. It is the failure you'd expect when the Markovian property does not hold for a given task. If the majority of training data in a given context (e.g. robot in home position, grasped drawer handle) corresponds to no motion, \( \pi_0 \) will always output "zero joint velocity"—and get stuck.
          </p>
          
          

        
          <div class="columns">
            <div class="column is-half">
              <p><strong>3. Token Decoding Edge Cases</strong></p>
              <p>
                During inference, \( \pi_0 \) will throw out this error: <code>Error decoding tokens: cannot reshape array of size 79 into shape (8)</code>
              </p>
              <p>
                According to our discussion on <a href="https://github.com/Physical-Intelligence/openpi/issues/373">Github Issue#373</a>, sometimes the policy decoded mis-shaped actions occasionally during inference. 
                In official implementation, <strong>pi0-fast-droid</strong> defaults to "no-motion" in these cases. Because the robot continues querying the policy, 
                the error is skipped on subsequent queries, allowing the robot to quickly recover and continue decoding correctly-shaped outputs.
              </p>
              <p><em style="color: red;">Warning: Don't kill the \( \pi_0 \) Inference Server when early stops occur. Be careful as the robot may continue moving before the server restarts!</em></p>
            </div>
            <div class="column is-half">
              <div>
                <img src="static/images/eval_Articulation_open the upper drawer_visualization.png" alt="Drawer articulation visualization" width="100%" style="border-radius: 10px;">
                <p class="is-size-6 has-text-centered">A visualization of Robot Joint and Gripper states during early stops</p>
              </div>
            </div>
          </div>
        
        </div> <!-- End of 4.1 -->





        <!-- Imprecise spatial reasoning -->
      <h3 class="title is-4">4.2 Imprecise spatial reasoning</h3>
        <div class="content has-text-justified">
          <p>
            \( \pi_0 \) often struggles with spatial reasoning about height. For example, when asked to pick up an object and place it into a container, the policy cannot lift the object high enough to clear the height of the container. This suggests one drawback of <strong>vision-based policies</strong>: the policy does not have a metrically accurate method to determine the distance between the gripper and the surrounding environment. 
          </p>

          <div class="columns">
            <div class="column is-half">
              <p>
              As shown here, the robot seems to think the gripper is high enough, and so it pushes the destination container when it tries to place the object into it. Existing methods using monocular RGB images are able to accurately estimate the size of an object, and the distance between the height of the object and the bowl. The model should be able to understand that if it could increase the height of the object relative to the container, it could complete the task successfully.
              </p>
            </div>

            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/Pushing_Bowl_Fail.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the pink bowl" - \( \pi_0 \) fails to lift the bottle high enough and always it collides with the bowl</p>
              </div>
            </div>
        </div>

        <div class="columns">
          <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.2_touch_the_index_finger_of_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Touch the index finger of outstretched hand" - \( \pi_0 \) correctly touches the finger, but it uses the strong force of the gripper</p>
              </div>
            </div>
            <div class="column is-half">
              <p>
                We also tried to prompt \( \pi_0 \) to raise the gripper higher (i.e. "raise the bottle high enough / up 10cm to avoid collision..."), but this did not help.
                When \( \pi_0 \) is asked to operate with an articulated object, it becomes even harder for it to estimate the distance from the side view camera, causing frequent collisions. This is particularly worth noting when the robot is interacting with humans. Because the robot has no safety constraints,  it will sometimes accidentally hit / grasp the user's hand, which could hurt the user! 
              </p>
            </div>
          </div>

          <p>
            What's more, when \( \pi_0 \) is told to manipulate a household appliance that it did not see during training, it will tend to collide with the device or stop during trials. As shown below, \( \pi_0 \) can not use the coffee machine in our lab.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_take_out_the_cup_under_the_coffee_machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Take out the cup under the coffee machine" → collides with top of machine</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_raise the lever of coffee machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Raise the lever of coffee machine" → didn't understand where the lever is.</p>
              </div>
            </div>
          </div>

          <p>
            One possible solution is to use techniques such as voxel maps and planning constraints. Including depth information with a depth camera could also be helpful to implement collision avoidance.
          </p>
        </div>
<!-- 
      <h3 class="title is-4">4.3 Sensitivity to Environment Change</h3>
        <div class="content has-text-justified">
          <p>
            It is well-known that most robot policies are sensitive to environment change. Change the camera slightly or the initial position of the objects and the success rate can plummet. We found that \( \pi_0 \) displays some robustness, but performance can be dramatically improved by moving the test distribution closer to the training dataset.
          </p>

          <h5 class="title is-6">1. Orientation of objects significantly affects performance</h5>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_place_the_can_into_red_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the can into the red tray" - Success rate varies with can orientation</p>
              </div>
            </div>
            <div class="column is-half">
              <p>
                As shown, when the spam can is parallel to the viewer, the robot fails consistently. When perpendicular, it succeeds consistently. This orientation bias likely stems from the viewpoint distribution in the training data, where certain viewpoints were overrepresented.
              </p>
            </div>
          </div> -->

<!-- 
          <p>
            Potential solutions might involve augmenting behavior cloning with:
          </p>

          <ul>
            <li>Explicit object-centric representations to identify functional parts</li>
            <li>Adversarial training to improve robustness to environment change</li>
            <li>Incorporating physical priors about stable grasps and object affordances</li>
          </ul> -->
        </div>
    </div>
    
</section>


<!-- Quirks Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">5. Quirk: Some interesting behaviors of pi0</h2>

        <h4 class="title is-5">Quirk 1: Prompt Engineering matters</h4>
        <div class="content has-text-justified">
          <p>
            We investigated how variations in the prompts provided affect the policy's behavior, and found that pi0's performance heavily depends on the instructions given by the user, leaving space for prompt engineering.
          </p>

          <h5 class="title is-6">1.1 You need to tune your prompt carefully to operate the robot</h5>
          \( \pi_0 \) freezes or fails when instructions contain typos, grammatical errors, or ambiguous phrasing. For example, when we try to let \( \pi_0 \) manipulate the articulated objects, we may need to try multiple different prompts to find the <em>'in-distribution'</em> instructions.

          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Instruction</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>"Close the toilet"</em></td>
                <td>0%</td>
                <td>Wanders aimlessly, unable to localize the target.</td>
              </tr>
              <tr>
                <td><em>"Close the white lid of the toilet"</em></td>
                <td>100%</td>
                <td>Always closes the toy toilet.</td>
              </tr>
            </tbody>
          </table>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\quirk\close_the_white_lid_of_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the white lid for the toilet (Success)</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\quirk\close_the_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the toilet (Failure)</p>
              </div>
            </div>
          </div>

          <h5 class="title is-6">1.2 pi0's behavior without language goals</h5>
          
          <p>
            When given no specific language instruction, \( \pi_0 \) defaults to interacting with the most familiar objects from its training data:
          </p>

          <ul>
            <li>Given nonsense text like <em>"dgbfzjkfhjilawhdfkAWHDKLWHADFiQAWFHqawipfjcasklfmdc"</em>, it picks up marker pens</li>
            <li>Given <em>"xxx"</em>, it reaches for blocks repeatedly</li>
          </ul>

          <p>
            In the DROID dataset, marker pens comprise 16.67% of objects, which could influence \( \pi_0 \) to pick the pen up when it is only given vision guidance.  Default behaviors are heavily influenced by training data distribution. Overcoming this ambiguity and rejecting invalid instructions is still an ongoing problem. 
          </p>


        <div class="columns is-multiline is-centered">
          <div class="column is-half">
            <div>
              <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                <source src="static\videos\quirk\nonsense-pen.mp4" type="video/mp4">
              </video>
              <p class="is-size-6 has-text-centered">dgbfzjkfhjilawhdfkAWHDKLWHADFiQAWFHqawipfjcasklfmdc<br>(nonsense, always pick up pen)</p>
            </div>
          </div>
          <div class="column is-half">
            <div>
              <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                <source src="static\videos\quirk\nonsense-block.mp4" type="video/mp4">
              </video>
              <p class="is-size-6 has-text-centered">xxx<br>(nonsense, the policy will back and forth)</p>
            </div>
          </div>
        </div>


        <h4 class="title is-5">Quirk 2: How robust is \( \pi_0 \) under partial observability?</h4>
        <div class="content has-text-justified">
          <p>
            One of the most frequently asked questions about \( \pi_0 \) is, how robust is it when its visual inputs are disrupted? We ran several tests on blocking the camera and the object.
          </p>

          <h5 class="title is-6">Camera Blocking Experiments</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task: "Pick up the pink object and place it into the bowl."</strong></li>
            <li><strong>Cameras</strong>: Side-view (primary) + wrist-mounted (secondary).</li>
            <li><strong>Blocking Scenarios</strong>: Partial/full occlusion of one or both cameras.</li>
            <li><strong></strong>Test time</strong>: 4 trials per scenario, 300 steps of rollouts per trial.</li>
          </ul>


          <div class="column is-full-width">
            <div>
              <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                <source src="static/videos/quirk/AP_block_leftcamera_pick_up_the_purple_object.mp4" type="video/mp4">
              </video>
              <p class="is-size-6 has-text-centered"><strong>Block the left camera:</strong> \( \pi_0 \) can still find pink object.</p>
            </div>
          </div>

          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Blocking Type</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>No Block</strong></td>
                <td>100%</td>
                <td>Baseline: Perfect execution. Picks correct object, then explores others.</td>
              </tr>
              <tr>
                <td><strong>Block Side Camera Mid-Trial</strong></td>
                <td>50%</td>
                <td>Relies on wrist camera to pick up the object → success rate decreases.</td>
              </tr>
              <tr>
                <td><strong>Block Wrist Camera Entirely</strong></td>
                <td>0%</td>
                <td>Frozen—no recovery.</td>
              </tr>
              <tr>
                <td><strong>Block Both Cameras Initially → Unblock Mid-Trial</strong></td>
                <td>75%</td>
                <td>Chaotic exploration → can execute after unblocking.</td>
              </tr>
            </tbody>
          </table>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_0.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>No Block:</strong> 100% execution.</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_1.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Side Camera Blocked Mid-Trial:</strong> Robot is interrupted but can still complete the task.</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_2.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Wrist Camera Blocked Entirely:</strong> Frozen.</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_3.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Both Blocked, then Unblocked:</strong> Recovers.</p>
              </div>
            </div>
          </div>


          <h5 class="title is-6">Object Blocking Experiment</h5>
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the pineapple."</li>
            <li><strong>Occlusion Levels</strong>: None (fully visible), 50% occluded, 100% occluded.</li>
            <li><strong>Test time</strong>: 12 trials per scenario, 300 rollouts per trial.</li>
          </ul>

            <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_box_pick_up_the_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>From three boxes: 66.67%</strong> \( \pi_0 \) can easily find the target object from different boxes on the table.</p>
              </div>
            </div>
            <!-- <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_box_moving_pick_up_the_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>From Boxes, but moving:</strong>Switching the boxes to make test more challenging.</p>
              </div>
            </div> -->

            <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_drawer_find_pineapple_inside_drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Inside Drawer: 25%</strong> 
                  <br>
                  <em>Franka's collision with the drawer makes \( \pi_0 \) hard to pick out the pineapple.</em>
                </p>
              </div>
            </div>
  

            <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_interactive_pick_the_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Hidden under cloth: 0%</strong>
                  <br>
                  <em>\( \pi_0 \) is unable to explore the environment that requires interaction exploration:.</em>
                </p>
              </div>
            </div>

          
  
        </div>


        <h5 class="title is-6">Our Observations</h5>
        <ul>
          <li><strong>Active Perception</strong>:
            <ul>
              <li>When the side camera is blocked, \( \pi_0 \) can still complete the task using only the wrist camera.</li>
              <li>In most pick and place tasks, \( \pi_0 \) relies on the wrist camera, as the side-view camera may be too far.</li>
              <li>The wrist camera facilitates active perception, exploring the scene and providing the policy information on whether a grasp would be successful at any given time. </li>
            </ul>
          </li>
          <li><strong>Viewpoint Robustness</strong>:
            <ul>
              <li>\( \pi_0 \) can tolerate changes to the side-view camera position and orientation during the task</li>
              <li>If the camera is blocked and then unblocked, \( \pi_0 \) can recover.</li>
            </ul>
          </li>
          <li><strong>Common Failure Modes under Partial Observability</strong>:
            <ul>
              <li>Total wrist camera blockage → robot freezes and cannot execute, even if the side image is available.</li>
              <li>\( \pi_0 \) is memoryless, predicting the action per frame auto-regressively, so it will be able to continue executing the task if observation is available.</li>
              <li>However, the exploration behavior of \( \pi_0 \) is not efficient and is limited to certain areas of the scene. This makes it difficult to actively search the environment.</li>
            </ul>
          </li>
        </ul>
        
      </div>
    </div>
  </div>
</section>


<!-- Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conclusion</h2>

        <div class="content has-text-justified">
          <p>Through our evaluations, we conclude that \( \pi_0 \) is a useful generalist policy, capable of intelligent behavior in unseen manipulation tasks. For our lab, \( \pi_0 \) will serve as a bedrock layer to build specific functionality on top of, as well as a baseline for comparison.  However, many challenges remain - our experiments showed that Pi0's performance is sensitive to its prompts, and that the policy struggles with instruction following, fine-grained tasks and partial observability. We are optimistic that these areas will be improved by the robot learning community, and believe that generalist policies will be widely used in the future.</p>     
          <!-- <div class="buttons is-centered mt-5">
            <a href="#" class="button is-link is-medium">
              <span class="icon">
                <i class="fas fa-table"></i>
              </span>
              <span>Explore Full Results</span>
            </a>
            
            <a href="#" class="button is-success is-medium">
              <span class="icon">
                <i class="fas fa-video"></i>
              </span>
              <span>Watch Full Videos</span>
            </a> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- References -->
<section class="section" id="references">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">References</h2>
        <div class="content">
          <div class="csl-bib-body">
            <!-- 1. \( \pi_0 \) -->
            <div class="csl-entry">
              K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, et al. (2024).
              Black, o., Xia, Z., Ha, L., Kaplan, A., Huang, H., Hausman, K., Ichter, B., Fox, D., & Levine, S. (2024). 
              <em><span class="paper-title">π0: A vision-language-action flow model for general robot control.</span></em>
              <span class="paper-conference">arXiv preprint arXiv:2410.24164.</span>
              <a href="https://arxiv.org/abs/2410.24164">[Paper]</a>
              <a href="https://www.pi.website/blog/pi0">[Project Page]</a>
            </div>
            
            <!-- 2. FAST -->
            <div class="csl-entry">
              K.&nbsp;Pertsch, K.&nbsp;Stachowicz, B.&nbsp;Ichter, D.&nbsp;Driess, S.&nbsp;Nair, Q.&nbsp;Vuong, O.&nbsp;Mees, C.&nbsp;Finn, &amp; S.&nbsp;Levine. (2025).
              <span class="paper-title"><em>FAST: Efficient action tokenization for vision-language-action models.</em></span>
              <span class="paper-conference"><em>arXiv&nbsp;preprint</em> arXiv:2501.09747.</span>
              <a href="https://arxiv.org/abs/2501.09747">[Paper]</a>
              <a href="https://www.pi.website/research/fast">[Project&nbsp;Page]</a>
            </div>

            <!-- 9. OpenPI repository -->
            <div class="csl-entry">
              Physical-Intelligence. (2025).
              <span class="paper-title"><em>OpenPI.</em></span>
              <span class="paper-conference"><em>GitHub&nbsp;repository.</em></span>
              <a href="https://github.com/Physical-Intelligence/openpi">[GitHub]</a>
            </div>

            <!-- 3. DROID dataset -->
            <div class="csl-entry">
              A.&nbsp;Khazatsky, K.&nbsp;Pertsch, S.&nbsp;Nair, A.&nbsp;Balakrishna, S.&nbsp;Dasari, S.&nbsp;Karamcheti, S.&nbsp;Nasiriany, M.&nbsp;K.&nbsp;Srirama, L.&nbsp;Y.&nbsp;Chen, K.&nbsp;Ellis, et&nbsp;al. (2024).
              <span class="paper-title"><em>DROID: A large-scale in-the-wild robot manipulation dataset.</em></span>
              <span class="paper-conference"><em>arXiv&nbsp;preprint</em> arXiv:2403.12945.</span>
              <a href="https://arxiv.org/abs/2403.12945">[Paper]</a>
            </div>

            <!-- 4. Chatbot Arena -->
            <div class="csl-entry">
              W.-L.&nbsp;Chiang, L.&nbsp;Zheng, Y.&nbsp;Sheng, A.&nbsp;N.&nbsp;Angelopoulos, T.&nbsp;Li, D.&nbsp;Li, B.&nbsp;Zhu, H.&nbsp;Zhang, M.&nbsp;I.&nbsp;Jordan, J.&nbsp;E.&nbsp;Gonzalez, &amp; I.&nbsp;Stoica. (2024).
              <span class="paper-title"><em>Chatbot Arena: An open platform for evaluating LLMs by human preference.</em></span>
              <span class="paper-conference"><em>Proceedings of the 41st International Conference on Machine Learning</em> (ICML 2024).</span>
              <a href="https://arxiv.org/abs/2403.04132">[Paper]</a>
              <a href="https://chat.lmsys.org">[Platform]</a>
            </div>
            <!-- 10. Interconnects blog post -->
            <div class="csl-entry">
              N.&nbsp;Lambert. (2024).
              <span class="paper-title"><em>ChatBotArena: The peoples' LLM evaluation, the future of evaluation, the incentives of evaluation, and gpt2chatbot.</em></span>
              <span class="paper-conference"><em>Interconnects.ai&nbsp;blog.</em></span>
              <a href="https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation">[Article]</a>
            </div>
      

            <!-- 5. Conceptual Captions -->
            <div class="csl-entry">
              P.&nbsp;Sharma, N.&nbsp;Ding, S.&nbsp;Goodman, &amp; R.&nbsp;Soricut. (2018).
              <span class="paper-title"><em>Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.</em></span>
              <span class="paper-conference"><em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em> (ACL 2018), 2556 - 2565.</span>
              <a href="https://aclanthology.org/P18-1238/">[Paper]</a>
            </div>

            <!-- 6. All You May Need for VQA are Image Captions -->
            <div class="csl-entry">
              S.&nbsp;Changpinyo, D.&nbsp;Kukliansy, I.&nbsp;Szpektor, X.&nbsp;Chen, N.&nbsp;Ding, &amp; R.&nbsp;Soricut. (2022).
              <span class="paper-title"><em>All you may need for VQA are image captions.</em></span>
              <span class="paper-conference"><em>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
              Human Language Technologies</em> (NAACL 2022), 1947 - 1963.</span>
              <a href="https://aclanthology.org/2022.naacl-main.142/">[Paper]</a>
            </div>

            <!-- 7. Open Images V4 -->
            <div class="csl-entry">
              A.&nbsp;Kuznetsova, H.&nbsp;Rom, N.&nbsp;Alldrin, J.&nbsp;Uijlings, I.&nbsp;Krasin, J.&nbsp;Pont-Tuset, S.&nbsp;Kamali, S.&nbsp;Popov, M.&nbsp;Malloci, A.&nbsp;Kolesnikov, T.&nbsp;Duerig, &amp; V.&nbsp;Ferrari. (2020).
              <span class="paper-title"><em>The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale.</em></span>
              <span class="paper-conference"><em>International Journal of Computer Vision</em>, 128, 1956 - 1981.</span>
              <a href="https://doi.org/10.1007/s11263-020-01316-z">[Paper]</a>
              <a href="https://storage.googleapis.com/openimages/web/index.html">[Dataset]</a>
            </div>

            <!-- 8. PaliGemma -->
            <div class="csl-entry">
              L.&nbsp;Beyer, A.&nbsp;Steiner, A.&nbsp;S.&nbsp;Pinto, A.&nbsp;Kolesnikov, X.&nbsp;Wang, D.&nbsp;Salz, M.&nbsp;Neumann, I.&nbsp;Alabdulmohsin, M.&nbsp;Tschannen, E.&nbsp;Bugliarello, et&nbsp;al. (2024).
              <span class="paper-title"><em>PaliGemma: A versatile 3B VLM for transfer.</em></span>
              <span class="paper-conference"><em>arXiv&nbsp;preprint</em> arXiv:2407.07726.</span>
              <a href="https://arxiv.org/abs/2407.07726">[Paper]</a>
            </div>

     
      

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Disclaimer Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="box has-background-warning-light">
          <h3 class="title is-4">Disclaimer</h3>
          <div class="content has-text-justified">
            <p>
              Our evaluations were conducted using the <strong>π₀-FAST-DROID</strong> model, specifically fine-tuned on <strong>DROID robot setups</strong>. While the model supports prompt-based control, its <strong><i>"LLM-like-download-and-prompt"</i></strong> functionality is currently limited to <strong>embodiments similar to those seen during training</strong>—particularly setups included in the <strong>Distributed RObot Interaction Dataset (DROID)</strong>.
            </p>
            <p> 
              The evaluation is conducted from Feb. 22 to March. 22, with a focus on limited environment and tasks type. Please check the Appendix section for more details on results. 
            </p>
            <p>
              PennPAL Research Group also contributed data to the DROID dataset in late 2023, which means our setup is close to the distribution of the data \( \pi_0 \) was trained on. However, our setup has changed significantly since late 2023. These changes include camera positions, robot height and calibration, new objects, lighting, etc. All results, observations, and analyses presented in this blog are based solely on  <strong>our own rollouts</strong>, and <strong>do not reflect any official performance</strong> guarantees from the model authors or dataset maintainers.
            </p>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Acknowledgments -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Acknowledgments</h2>
        <div class="content has-text-justified">
          <p>
            We are grateful to Will Liang, Hungju Wang, and Sam Wang for their assistance in setting up the π0 environment. We further thank Kaustubh Sridhar, Tianyou Wang, Ian Pedroza, Ethan Yu, Tim Song, and Yiqian Li for their help doing the experiments.
          </p>
          <p>
            We also thank Junyao Shi, Aurora Qian, Leon Kim, and Jason Ma for their insightful suggestions on evaluating a generalist manipulation policy.
          </p>
          <p>
            We extend our appreciation to Karl Pertsch from Physical Intelligence for his constructive feedback on the blog draft.
          </p>
          <p>
            This work was conducted independently within the PennPAL and Daniilidis groups at the GRASP Lab.
          </p>
          <p>
            This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No DGE-2236662. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Citation change site-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <p>If you find this evaluation useful for your research, please consider citing our repository:</p>
          <pre><code>@misc{pi0-experiment-wild,
  author = {J.&nbsp;Wang*, M.&nbsp;Leonard, K.&nbsp;Daniilidis, D.&nbsp;Jayaraman, &amp; E.&nbsp;S.&nbsp;Hu},
  title = {Evaluating \( \pi_0 \) in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies},
  year = {2025},
  publisher = {GitHub},
  url = {https://penn-pal-lab.github.io/pi0-Experiment-in-the-Wild}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Conclusion Image -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <figure class="image">
            <img src="static/images/robot_workspace.jpg" alt="Robot Workspace">
            <figcaption>Our robot workspace setup used for evaluating pi0.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Robot & Model Setup Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Appendix A: Our Robot & Model Setup</h2>

        <div class="content has-text-justified">
          <p>The following are details of our experiment set up.</p>
        </div>
        
        <h3 class="title is-4">Hardware:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Franka Research 3 Arm</strong>: 7-DOF force-sensitive robot with a 3 kg payload.</li>
            <li><strong>Robotiq 2F-85 gripper:</strong> two-finger gripper with 5mm stroke and adjustable force control.</li>
            <li><strong>Cameras</strong>:
              <ul>
                <li><strong>Side-view:</strong> ZED 2 stereo camera for global scene understanding</li>
                <li><strong>Wrist-mounted:</strong> ZED Mini for close-range object manipulation</li>
                <li><strong>Perception Mode:</strong> <strong>Pure RGB</strong> (no depth calibration)</li>
              </ul>
            </li>
          </ul>
          
      
      </div>
        
        <h3 class="title is-4">Compute</h3>
        <div class="content has-text-justified">
          <p><strong>GPU Server:</strong></p>
          <ul>
            <li><strong>GPUs:</strong> 1x NVIDIA RTX A6000 (48GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.3</li>
            <li><strong>Usage:</strong> VLA model inference.</li>
          </ul>
          
          <p><strong>Workstation</strong></p>
          <ul>
            <li><strong>GPU:</strong> NVIDIA GeForce RTX 3080 (16GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.6</li>
            <li><strong>Usage:</strong> DROID low level control.</li>
          </ul>
        </div>
        
        <h3 class="title is-4">3.3 pi0-FAST-DROID:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Vision-Language Model</strong>: <em>Paligemma 3B</em> for spatial and semantic understanding.</li>
            <li><strong>FAST+</strong>: Frequency-space Action Sequence Tokenization (FAST), a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies.
            <li><strong>Training Data</strong>: Pretrained on π cross-embodiment robot dataset & Open X-Embodiment, fine tuned on DROID dataset.</li>
          </ul>
<!--           
          <figure class="image">
            <img src="static/images/dataset.jpg" alt="Large scale robotics data for pretraining">
            <figcaption>Dataset used for pretraining pi0</figcaption>
          </figure> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Appendix B: Detailed Results for Each Task</h2>

        <!-- Overall Results -->
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="box">
              <h3 class="title is-4">Overall Performance</h3>
              <div class="columns">
                <div class="column is-half">
                  <figure class="image">
                    <div class="columns">
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-scatter.png" alt="Scatter plot showing performance distribution">
                      </div>
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-bar.png" alt="Chart showing overall performance statistics">
                      </div>
                    </div>
                    <figcaption>Performance across 240+ trials (52% average progress)</figcaption>
                  </figure>
                </div>
                <div class="column is-half">
          <div class="content">
                    <p>Across our 240+ test trials, \( \pi_0 \) achieved varying degrees of success:</p>
                    <ul>
                      <li><strong>Complete Success:</strong> 38%</li>
                      <li><strong>Partial Success:</strong> 28%</li>
                      <li><strong>Complete Failure:</strong> 34%</li>
            </ul>
                    <p>We observed that performance varied significantly based on task type, environmental conditions, and most importantly, the phrasing of instructions.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
        </div>
        
        <!-- Task Specific Results -->
        <h3 class="title is-4">Task-Specific Performance</h3>
        
        <!-- Pick and Place -->
        <div class="box">
          <h4 class="title is-5">Pick-and-Place (44% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Familiar objects (e.g., pineapple toy, markers) - 90% success</li>
                  <li>Clear spatial targets ("in the pink bowl") - 85% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Large objects (black cube) - 35% success</li>
                  <li>Vague targets ("inside the bowl") - 40% success</li>
                  <li>Multi-object tasks ("Put all objects into the basket") - 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <figure class="image">
                <img src="static/images/pick_and_place_chart.jpg" alt="Chart showing pick and place performance">
              </figure>
            </div>
          </div>
        </div>
        
        <!-- Human Interaction -->
        <div class="box">
          <h4 class="title is-5">Human Interaction (53.5% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Object handovers - 46.67% success</li>
                  <li>Following a moving human - 65% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Precision interactions ("Shake hands") - 30% success</li>
                  <li>Recovery after interruption - 40% success</li>
                </ul>
              </div>
            </div>
      
            <div class="column is-4">
              <div class="columns is-multiline">
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\success\human_3_Pick up the pineapple and give it to the programmer.mp4" type="video/mp4">
      </video>
                  <p class="is-size-6 has-text-centered">"Give the pineapple to the programmer"</p>
    </div>
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\failure\human_7_Pick up the whiteboard eraser and give it to the programmer.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Give the whiteboard eraser to the programmer"</p>
  </div>
              </div>
            </div>

          </div>
        </div>
        

        <!-- Coffee Machine -->
        <div class="box">
          <h4 class="title is-5">Coffee Machine (8.00% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Capsule Coffee Machine:</strong></p>
                <ul>
                  <li>Close the capsule lid of coffee machine - 50% success</li>
                  <li>Pick up the capsule from the coffee machine - 0% success</li>
                  <li>Place the capsule into the coffee machine - 0% success</li>

                </ul>
                <p><strong>Espresso Coffee Machine:</strong></p>
                <ul>
                  <li>Pick up the coffee portafilter - 0% success</li>
                  <li>Pour the coffee into the cup - 0% success</li>
                  <li>Pick up the silver milk frothing pitcher- 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <div class="columns">
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/failure/coffee_failure.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Place the capsule into the coffee machine"</p>
                </div>
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/right_Close_the_capsule_lid_of_the_coffee_machine.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Close the capsule lid of the coffee machine"</p>
                </div>
              </div>
            </div>
          </div>
        </div>
        
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>\( \pi_0 \) Evaluation</strong> by <a href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">GRASP Lab</a>, University of Pennsylvania.
      </p>
      
      <p>
        The website template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
</footer>

</body>
</html>
