<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating Pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies">
  <meta name="keywords" content="Pi0, Robotics, VLA, Vision-Language-Action, PAL, GRASP Lab">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Evaluating Pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://grasp.upenn.edu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://grasp.upenn.edu/">
            GRASP Lab
          </a>
          <a class="navbar-item" href="https://droid-dataset.github.io/droid/">
            DROID Dataset
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Evaluating Pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://everloom-129.github.io/">Jie Wang</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="#">Matthew Leonard</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://edwardshu.com/">Edward S. Hu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Pennsylvania</span>
          </div>
    
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper links -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.09747" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Pi0-FAST Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://droid-dataset.github.io/droid/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>DROID Dataset</span>
                </a>
              </span>
              <!-- GitHub link -->
              <span class="link-block">
                <a href="https://github.com/Physical-Intelligence/openpi" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Pi Website</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="static/videos/success/success_novel_fold_newspaper.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        "Fold up the newspaper"
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Pi0 is a state-of-the-art vision-language-action (VLA) model designed for general-purpose robotic manipulation. 
            Built on internet-scale pre-training and the Open X-Embodiment dataset, Pi0 promises versatility across tasks 
            like pick-and-place, articulation, dexterity and human robot interaction. But how well does it actually perform in the wild?
          </p>
          <p>
            We are from GRASP Lab at the University of Pennsylvania. As contributors to the 
            <strong>Distributed RObot Interaction Dataset (DROID)</strong> dataset, we are fortunate to have early access to the PI0 model. 
            Our robot setup can be found <a href="https://droid-dataset.github.io/droid/docs/hardware-setup">here</a>. With no training, 
            no finetuning, we just deploy the <em>PI0-FAST-DROID</em> and use it zero-shot. In order to build upon this powerful model, 
            we largely test its performance on a variety of complex tasks, through pi0 into difficult scenes in the wild.
          </p>
          <p>
            Similar to <a href="https://en.wikipedia.org/wiki/Vibe_coding">Vibe Coding</a>, the philosophy of our evaluation is: more in-the-wild, less controlled / lab setup. We observe pi0 like biologists observing a new animal - conducting interesting tests, looking for broad, qualitative properties. The aim is to encourage others to try out pi0, such an open-source, amazing foundation model for manipulation.
          </p>
          <p>
            In this blog, we share results from <strong>300+ trials</strong> testing Pi0 on a Franka Research 3 robot. 
            We explore its strengths, unexpected quirks, and limitationsâ€”and what this means for the future of imitation learning.
            For detailed metrics and analysis, see our <a href="#results">Results & Insights</a> section.
          </p>
          <p>
            <strong>Key words:</strong> pi0, VLAs, Manipulation, Robot Learning, Imitation Learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Box -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="box has-background-light">
          <h3 class="title is-4 has-text-centered">TL;DR</h3>
          <div class="content">
            <ul class="is-size-5">
              <li><strong>Overall Success:</strong> 42.3% average task completion across 300+ trials</li>
              <li><strong>Key Finding:</strong> Pi0 demonstrates impressive vision-language understanding but struggles with spatial reasoning and precise manipulation</li>
              <li><strong>Bottom Line:</strong> Pi0 works impressively in the wild without fine-tuning, but success depends heavily on instruction phrasing and object familiarity</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Images Section -->
<section class="hero">
  <div class="container" style="width: 50%; max-width: 50%;">
    <div class="hero-body" style="padding: 1.5rem 1.5rem;">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <!-- First row - three images side by side -->
            <div class="columns is-vcentered is-gapless" style="margin-bottom: 1rem;">
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/lab_setup.jpg" alt="GRASP Lab Setup" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
              </div>
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/robot_workspace.jpg" alt="Robot Workspace" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
              </div>
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/lab_setup_2.jpg" alt="Levine Setup 2" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
              </div>
            </div>
            
            <!-- Second row - manipulation scene (full width) -->
            <div class="columns is-centered" style="margin-top: 0.5rem;">
              <div class="column is-12">
                <figure class="image">
                  <img src="static/images/wild-newsppaper.png" alt="Three Camera View" style="width: 100%;">
                </figure>
              </div>
            </div>
            
            <p class="has-text-centered" style="margin-top: 1rem;">
              Our evaluation setup at GRASP Lab, showing the robot workspace, test objects, and typical manipulation scenes.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Key Findings Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Findings</h2>
        <div class="content has-text-justified">
          <p>
            Through experiments across diverse manipulation tasks in the <a href="https://facilities.upenn.edu/maps/locations/levine-hall-melvin-and-claire-weiss-tech-house">Levine Hall</a> at Penn Engineering School, we observed a wide range of behaviorsâ€”remarkable performances, confusing failures, and many quirks. By testing Pi0 in varied environments (e.g., cluttered tables, articulated cabinets, human-involved settings), we concludes three critical insights:
          </p>

          <ol>
            <li>
              <strong>Pi0 Works (Mostly)</strong>: It achieves <strong>42.3% average progress</strong> across diverse, hard tasks, even in unseen environments and objects. However, it also fails in some seemingly simple tasks.
            </li>
            <li>
              <strong>Prompt Engineering Matters</strong>: While Pi0 is good at vision-action grounding, its <strong>performance drops 20~100%</strong> based on instruction phrasing. You need to carefully prompt it to make it work in some condition.
            </li>
            <li>
              <strong>Unexpected Quirks</strong>: Pi0 can recover from failures, handle moving humans in the scene, but struggles with <strong>mid-task freezing, collision avoidance, and fine-grained manipulation</strong>.
            </li>
          </ol>
          
          <p>
            Below, we unpack each finding with video examples and analysis. For detailed metrics, see <a href="#results">Section 4: Results</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Hero Images -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <div class="columns">
            <div class="column is-half">
              <img src="static/images/OverallResult.png" alt="Scatter plot showing performance distribution">
            <figcaption style="text-align: center;">Performance across 300+ trials (42.3% average progress)</figcaption>
            </div>
            <div class="column is-half">
              <img src="static/images/worldcloud.png" alt="Word cloud visualization">
            <figcaption style="text-align: center;">Word cloud of task instructions</figcaption>
          </div>

          </div>
        </figure>
      </div>
    </div>
  </div>
</section>



<!-- Success Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Success Cases</h2>

        <!-- Success videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Pick and Place</h3>
                  <div>
                    <video id="success-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_pickplace_fish_to_box.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Place the yellow fish into the purple box"</p>
                  <p class="is-size-6">Precise placement of camouflage fish into the box</p>
                </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Articulation</h3>
                  <div>
                    <video id="success-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_articulation_open_drawer.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Open the drawer"</p>
                  <p class="is-size-6">Opening drawer with multiple pull to make sure fully opened</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Human Robot Interaction</h3>
                  <div>
                    <video id="success-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_hri_handover_pineapple.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Hand the pineapple to the programmer"</p>
                  <p class="is-size-6">Safe object handover to programmer, even there is wire occlusion from side view</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Dexterity</h3>
                  <div>
                    <video id="success-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_dexterity_pour_water.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pour water from the silver cup to the pink bowl"</p>
                  <p class="is-size-6">Pour real water from Latte art vat into the target bowl</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Multi-step Task</h3>
                  <div>
                    <video id="success-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_multistep_fill_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pick up all the objects into the basket"</p>
                  <p class="is-size-6">Sequential bagging all the toys into basket</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Novel Objects</h3>
                  <div>
                    <video id="success-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/right_Close_the_capsule_lid_of_the_coffee_machine_left.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Close the capsule lid of the coffee machine"</p>
                  <p class="is-size-6">Handling previously unseen device</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Remarkable Performance Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2.1 Remarkable Performance of PI0</h2>

        <!-- Vision-Language Understanding -->
        <h3 class="title is-4">2.1.1 Robust Vision-Language Understanding in Complex Scenes</h3>
        <div class="content has-text-justified">
          <p>
            Powered by <strong>PaliGemma</strong> (Google DeepMind's 3B VLM) as its vision encoder (<a href="https://arxiv.org/abs/2407.07726">Beyer etc, 2024</a>), Pi0 demonstrates robust scene comprehension and adaptability. Despite relying solely on uncalibrated monocular RGB inputs (224Ã—224 pixels after compression), it can handle very challenging objects and environments like transparent, camouflaged and novel items. Showing the potential of End2End VLA in precision, closed loop control.
          </p>
          
          <p><strong>1. It can grasp transparent object</strong></p>
          
          <p>
            PI0 is capable of identifying and manipulating transparent objects, as shown below. It picks up the bottle with a stable grasp, aligns it to the small cup, and precisely drops it in. Many traditional grasp detection techniques require an accurate 2D or 3D reconstruction of the scene, and transparent objects can cause issues in reconstruction accuracy. This makes it all the more impressive that the model can detect transparent objects solely from uncalibrated, mono-RGB views from the wrist camera and one side view camera.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\Success_Dropping_Into_Cup.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the white cup."</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\transparent_bottle_place.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the white cup."</p>
              </div>
            </div>
          </div>
          
  
          <p><strong>2. It can grasp an object even when it is camouflaged into a colorful background</strong></p>
          
          <p>
            PI0 can identify the 'yellow fish' here even when it is placed on top of a colorful board game. This object has an unusual and difficult shape, and it blends in well with the background, but pi0 detects well with its position, and its shape, well enough to grasp it up.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half"> 
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/success/camflage_yellow_fish.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the red box"</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_pickplace_fish_to_box.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the purple box"</p>
              </div>
            </div>
          </div>
          
          <p><strong>3. It is robust to human activity in the input</strong></p>
          
          <p>
            During experiments, there were many scenarios where the side-view camera captured humans moving around in the background. However, pi0 was still focused on the task, keeping the robotic arm's movements focused on object manipulation.
          </p>
          
          <p>
            We believe there are two reasons for pi0's robustness to human movement. First, the pre-trained VLM backbone of Pi0 is trained on human-involved images (<a href="https://aclanthology.org/P18-1238/">Sharma et al., 2018</a>, <a href="https://aclanthology.org/2022.naacl-main.142/">Changpinyo et al., 2022a</a>, <a href="https://storage.googleapis.com/openimages/web/factsfigures_v7.html">Kuznetsova et al. 2020</a>), so humans are in-distribution. Next, from our occlusion experiments in <strong>Section 2.3.1</strong>, the policy seems to prioritize the wrist camera's images during pick-and-place tasks, so distractors in the side-view camera seem to minimally affect the policy.
          </p>
          
          <p>
            Here are two side-view videos involving humans in the scene. Please refer to <strong>Section 4.6</strong> for more results on <strong>human-robot interaction</strong>.
          </p>
          
          <p><em>(All videos featuring humans were uploaded with permission from the individuals involved.)</em></p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_initial_PI0_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Pick the pineapple and place it into the basket"</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_hand_the_pineapple_to_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to the outstretched hand"</p>
              </div>
            </div>
          </div>
            

          <p>
            There's plenty of work in the past in CV, Robotics that does transparent object detection and manipulation. But the nice part here is that we have a data-driven system that does it, without any special logic or care for transparent objects.
          </p>
          <p class="box has-background-grey-lighter has-text-centered is-italic">
            "Pi0's ability to handle transparency, clutter and distractors hints at a future where robots see the world as humans doâ€”through semantics, not just pixels."
          </p>
          
        </div>
        
        <!-- High Frequency Dexterity -->
        <h3 class="title is-4">2.1.2 High frequency dexterity of robot policy</h3>
        <div class="content has-text-justified">
          <p>
            Even though pretrained on massive datasets, Pi0 can perform high-frequency, closed-loop control up to 50Hz. Despite a <strong>90ms-300ms delay</strong> introduced by HTTP connections between robot and the GPU server (<a href="https://x.com/svlevine/status/1886822326274285709">Sergey, 2025</a>), Pi0 achieves real-time responsiveness. It can do precise manipulation of complex objects like newspapers, T-shirts, and milk frothing pitcher.
          </p>
          
          <p>
            Here are some impressive cases demonstrating the dexterity and precision of pi0. Notice all videos in this blog are <strong>uncut and unedited</strong> from the ZED 2 camera at <strong>1x speed</strong>.
          </p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_remove_the_pink_bowl_from_the_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Remove the pink bowl from the tray"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_stack_the_wooden_blocks.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Stack the wooden blocks"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_fold_the_cloth_from_left_to_right.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold the cloth from left to right"</p>
              </div>
            </div>
          </div>
          
          <p>
            According to the PI0 paper and pi0-FAST (<a href="https://www.pi.website/blog/pi0">Kevin etc, 2024</a>, <a href="https://www.pi.website/research/fast">Karl etc, 2025</a>), we think the dexterity stems from its flow matching architecture. Recently there are some efforts to run VLA with 7B VLM encoder at 50Hz (<a href="https://arxiv.org/pdf/2502.19645">Kim etc, 2025</a>), which is very impressive. We look forward to more large models running at a higher frequency! 
          </p>
        </div>
        
        <!-- Failure Recovery -->
        <h3 class="title is-4">2.1.3 Failure recovery and generalization</h3>
        <div class="content has-text-justified">
          <p>
            One key difference between the pi0 and other models is: it can work in the wild, without online data collection, without calibration, without much position tuning on the policy, we can let it run on our franka robot directly.
          </p>
          
          <p>
            More interestingly, the policy behaves similar to diffusion policy, as shown below, it could recover from failures and retry tasks that initially failed.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\pick_banana.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Pick up the banana"</p>
              </div>
            </div>
          </div>

          

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Failiure Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Failture Case</h2>
        <p class="has-text-justified" style="color: #666; font-style: italic;">
          Pi0 demonstrates very impressive robustness across different lights, locations and tasks. However, it do exist some weakness, here is some examples on its cons:
        </p>
        <!-- Failure videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Objects</h3>
                  <div>
                    <video id="failure-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_beaker_into_the_pink_bowl.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Place the beaker into the pink bowl"</i></p>
                  <p class="is-size-6">Cannot find the transparent glass beaker</p>
                </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Background</h3>
                  <div>
                    <video id="failure-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\upenn_pick_block.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pick the black box on the white box"</i></p>
                  <p class="is-size-6">Can not handle well with unseen background</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Task Misunderstanding</h3>
                  <div>
                    <video id="failure-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_fish_into_the_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"place the yellow fish into the basket"</i></p>
                  <p class="is-size-6">Pick up the wrong object in cluttered scene (25% progress)</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Spatial Reasoning</h3>
                  <div>
                    <video id="failure-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_can_into_the_tray.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Place the can into the tray"</i></p>
                  <p class="is-size-6">Misjudges object position relative to container (19% error)</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">General Articulation</h3>
                  <div>
                    <video id="failure-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\video_close_the_right_cabinet_door.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Close the right cabinet door"</i></p>
                  <p class="is-size-6">Fails to open toy kitchen cabinet on the table (0% progress)</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Coffee Making</h3>
                  <div>
                    <video id="failure-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\coffee_pour_coffee_bean_into_grinder.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour coffee bean into the grinder"</i></p>
                  <p class="is-size-6">Cannot work with espresso machine (10% progress)</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Problems with PI0 Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2.2 Problem with PI0</h2>

        <!-- Early stopping problem -->
        <h3 class="title is-4">2.2.1 Early stopping</h3>
        <div class="content has-text-justified">
          <p>
            One common failure case is that policy may <strong>freeze unexpectedly during execution</strong>, leaving tasks incomplete until human intervention. 
            This behavior comes from two interrelated factors: semantic ambiguity and autoregressive action decoding limitations.
          </p>

          <h4 class="title is-5">Root Causes</h4>
          
          <p><strong>1. Semantic Misalignment</strong></p>
          <p>
            Pi0 lacks LLM-like commonsense reasoning to infer unfamiliar object categories. When it does not understand a command, it gets stuck. 
            In some experiments, we found that some objects were out of distribution(OOD), causing early stopping.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/failure/Fail_Gun_Stuck.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the gun into the blue box" - Pi0 stops because it can't recognize which object is 'gun'</p>
              </div>
            </div>
          </div>
          
          <p><strong>2. Autoregressive Local Optima</strong></p>
          <ul>
            <li><strong>Case</strong>: "Open the drawer" â†’ stops after grasping the handle.</li>
            <li><strong>Behavior</strong>: Pi0 assumes the task is complete once the handle is gripped, failing to infer multi-step articulation.</li>
            <li><strong>Why</strong>: The model predicts actions frame-by-frame without memory of prior steps.</li>
          </ul>
          
          <p><strong>3. Token Decoding Edge Cases</strong></p>
          <p>
            During inference, pi0 will throw out this error: <code>Error decoding tokens: cannot reshape array of size 79 into shape (8)</code>
          </p>
          <p>
            According to our discussion on <a href="https://github.com/Physical-Intelligence/openpi/issues/373">Github Issue#373</a>, sometimes the policy decoded mis-shaped actions occasionally during inference. 
            In official implementation, <strong>pi0-fast-droid</strong> defaults to "no-motion" in these cases. Since as the robot continues querying the policy, 
            the error gets skipped on subsequent queries, allowing the robot to quickly recover and continue decoding correctly-shaped outputs.
          </p>
        </div>
        
        <!-- Imprecise spatial reasoning -->
        <h3 class="title is-4">2.2.2 Imprecise spatial reasoning</h3>
        <div class="content has-text-justified">
          <p>
            Pi0 often struggles with spatial reasoning over height. For example, when asked to pick up an object and place it into a container, 
            the policy cannot lift the object high enough to clear the height of the container. This suggests the drawback of vision-based 
            autoregressive VLA: it does not have a metrically accurate method to estimate the distance between gripper and surrounding environment.
          </p>

          <div class="columns is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/Pushing_Bowl_Fail.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the pink bowl" - Pi0 fails to lift high enough</p>
              </div>
            </div>
          </div>

          <p>
            We also tried to prompt pi0 to raise the gripper higher, but it didn't learn on this, making it easy to collide with containers.
            When pi0 is asked to operate with articulated objects, it becomes even harder for it to estimate the distance from the side view camera, 
            causing frequent collisions. This is particularly concerning when the robot is interacting with humans, as it doesn't have an explicit 
            constraint map and may hit/grasp the user's hand, which is unsafe.
          </p>

          <p>
            What's more, when Pi0 is required to manipulate household appliances, it tends to collide with the device or stop during inference trials. 
            As shown below with our coffee machine examples:
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/failure/coffee_take_out_the_cup_under_the_coffee_machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Take out the cup under the coffee machine" â†’ collides with top of machine</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/failure/coffee_take_out_the_silver_cup_from_the_coffee_machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Take out the silver cup from the coffee machine" â†’ succeeds by grasping handle</p>
              </div>
            </div>
          </div>

          <p>
            One possible solution is to involve concepts like Vox map and planning constraints. Depth cameras would also be very helpful             to implement collision avoidance.
          </p>
        </div>
        


<!-- Quirks Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2.3 Quirk: Some interesting behavior of pi0</h2>

        
        <h4 class="title is-5">Quirk 1: Active Perception & Viewpoint Robustness</h4>
        <div class="content has-text-justified">
          <p>
            One of the most frequently asked questions on Pi0 is, how robust it is when the sensory inputs are disrupted? We did several tests about blocking the camera, the object and moving the side view camera. Here's what we learned.
          </p>
          
          <h5 class="title is-6">Camera Blocking Experiments</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the pink object and place it into the bowl."</li>
            <li><strong>Cameras</strong>: Side-view (primary) + wrist-mounted (secondary).</li>
            <li><strong>Blocking Scenarios</strong>: Partial/full occlusion of one or both cameras.</li>
          </ul>
          
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Blocking Type</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>No Block</strong></td>
                <td>100%</td>
                <td>Flawless execution. Picks pink object, attempts secondary objects.</td>
              </tr>
              <tr>
                <td><strong>Block Side Camera Mid-Trial</strong></td>
                <td>50%</td>
                <td>Relies on wrist cam. Gripper misalignment â†’ partial success.</td>
              </tr>
              <tr>
                <td><strong>Block Wrist Camera Entirely</strong></td>
                <td>0%</td>
                <td>Frozenâ€”no recovery.</td>
              </tr>
              <tr>
                <td><strong>Block Both Cameras Initially â†’ Unblock Mid-Trial</strong></td>
                <td>70%</td>
                <td>Chaotic exploration â†’ stabilizes after unblocking.</td>
              </tr>
            </tbody>
          </table>
          
          <h5 class="title is-6">Key Observations</h5>
          
          <ol>
            <li>
              <strong>Active Perception</strong>:
              <ul>
                <li>When the side camera is blocked, Pi0 uses its wrist camera to retry grasps ("hand-eye coordination").</li>
                <li>Example: In Trial 3 (side camera blocked), Pi0 adjusted its gripper position twice to finally secure the pink object.</li>
              </ul>
            </li>
            <li>
              <strong>Viewpoint Robustness</strong>:
              <ul>
                <li>Pi0 tolerates shifted camera angles mid-task.</li>
                <li>Blocking then unblocking cameras triggered exploratory movements (e.g., sweeping the arm to re-localize objects).</li>
              </ul>
            </li>
            <li>
              <strong>Failure Modes</strong>:
              <ul>
                <li>Total wrist camera blockage â†’ robot freezes (no fallback perception).</li>
                <li>Over-reliance on initial frames: If objects shift after blocking, Pi0 struggles to update its spatial model.</li>
              </ul>
            </li>
          </ol>
          
          <h5 class="title is-6">Object Blocking Experiment</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the red box."</li>
            <li><strong>Occlusion Levels</strong>: None (fully visible), 50% occluded, 100% occluded.</li>
            <li><strong>Behavior Metrics</strong>: Success rate, recovery attempts, and failure modes.</li>
          </ul>
          
          <p>
            We sought to determine how the robot would respond when the target of its task was occluded. In order to do this, we asked it to grasp a box, varying the occlusion level from fully visible, to 50%, to 100% occluded. When the box was fully visible, the robot would succeed. When it was 50% occluded, the robot would generally succeed, sometimes moving the occluding object in order to get a better view or grasp of the target. When it was fully occluded, though, the robot would become confused and stop. During one fully occluded trial, the robot knocked over the occluder and the red box, revealing more information about the location of the target but making it impossible for the robot to proceed with a good grasp.
          </p>
          
          <h5 class="title is-6">Why This Matters</h5>
          
          <p>
            Pi0's ability to <strong>improvise with partial observations</strong> hints at emergent active perceptionâ€”a critical feature for real-world deployment where lighting, occlusions, or camera failures are inevitable.
          </p>
          
          <p><em>Left: Blocking the side camera forces Pi0 to rely on wrist-view. Right: Success rates across blocking scenarios.</em></p>
          
          <p>
            This quirk underscores Pi0's potential as a resilient generalistâ€”though it's no substitute for dedicated SLAM or depth sensing... yet.
          </p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/camera_blocking_experiment.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Camera blocking experiment</p>
              </div>
            </div>
          </div>
        </div>
        

        
        <h4 class="title is-5">Quirk 2: Opening > Closing</h4>
        <div class="content has-text-justified">
          <p>
            Pi0 exhibits unexpected behaviors when interacting with articulated objects (e.g., drawers, cabinets), Pi0 achieves higher success rates when <strong>opening</strong> articulated objects (e.g., drawers) compared to <strong>closing</strong> them.
          </p>
          
          <p><strong>Examples</strong>:</p>
          
          <ul>
            <li>
              <strong>Task</strong>: "Close the drawer."
              <ul>
                <li><strong>Behavior</strong>: Pi0 grasps the handle, attempts complex maneuvers (e.g., twisting), and often fails to align with the articulation axis.</li>
                <li><strong>Result</strong>: <strong>60% success rate</strong> for opening vs. <strong>35% for closing</strong> (see video).</li>
              </ul>
            </li>
            <li>
              <strong>Toy Cabinet or Real Drawer on table Challenge</strong>:
              <ul>
                <li><strong>Task</strong>: "Close the toy kitchen cabinet."</li>
                <li><strong>Behavior</strong>: Pi0 freezes or pushes randomly when it is required to manipulate with articulated objects on table.</li>
                <li><strong>Result</strong>: <strong>0% success rate</strong> (see video).</li>
              </ul>
            </li>
          </ul>
          
          <p><strong>Possible Cause</strong>:</p>
          
          <ul>
            <li>Pi0's training data prioritizes <strong>opening actions</strong> (common in datasets like DROID).</li>
            <li>Closing requires precise backward articulation, which the model struggles to infer from RGB-only inputs.</li>
          </ul>
          
          <p>
            In general, closing things is much easier than opening. While pi0 tends to execute some more fancy actions to "close one thing". Like the robot will firstly grasp the drawer handle, then try to close it. While the closing task could be generally easy to move along the articulation axis.
          </p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\close_the_drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"> "Close the drawer"</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\close_the_right_cabinet_door.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"> "Close the right cabinet door"</p>
              </div>
            </div>
          </div>
          
          <p>
            For unfamiliar, toy-like cabinet doors, the closing becomes even harder. As pi0 is never trained on this articulation object, and it may tend to stop as it never learns how to close this type of object.
          </p>
        </div>
        
        <h4 class="title is-5">Quirk 3: Language Specificity Matters</h4>
        <div class="content has-text-justified">
          <p>
            Pi0's performance can be improved after tuning its instruction. Like this examples:
          </p>
          
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Instruction</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>"Close the toilet"</em></td>
                <td>0%</td>
                <td>Wanders aimlessly, unable to localize the target.</td>
              </tr>
              <tr>
                <td><em>"Close the white lid of the toilet"</em></td>
                <td>100%</td>
                <td>Precise alignment and execution.</td>
              </tr>
            </tbody>
          </table>
          
          <p><strong>Why This Happens</strong>:</p>
          
          <ul>
            <li>Vague instructions (e.g., <em>"toilet"</em>) force Pi0 to guess the target sub-component.</li>
            <li>Specificity (e.g., <em>"white lid"</em>) aligns with PaliGemma's object-part grounding capability.</li>
          </ul>
          
          <p><strong>Video Examples</strong>:</p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\close_the_white_lid_of_the_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the white lid for the toilet (Success)</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\failure\close_the_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the toilet (Failure)</p>
              </div>
            </div>
          </div>
          
          <p>
            On the contrary, Pi0 freezes or fails when instructions contain <strong>typos, grammatical errors, or ambiguous phrasing</strong>.
          </p>
          
          <p><strong>Example</strong>:</p>
          
          <ul>
            <li><strong>Instruction</strong>: <em>"Close the tiolet"</em> (misspelled).</li>
            <li><strong>Behavior</strong>: Robot hesitates, then picks up the nearest familiar object (e.g., a marker pen).</li>
            <li><strong>Implication</strong>: Pi0 lacks the linguistic robustness of LLMs like GPT-4, failing to infer intent from context.</li>
          </ul>
        </div>
        
        
        <!-- The Power of Prompts -->
        <div class="box">
          <h4 class="title is-5">The Power of Prompts</h4>
          <div class="columns">
            <div class="column is-half">
              <div class="content">
                <p>Pi0's 3B language model is very sensitive to phrasing:</p>
                <ul>
                  <li><strong>Good:</strong> "Put A into B" â†’ Clear spatial logic â†’ 80% success</li>
                  <li><strong>Bad:</strong> "Fill B with A" â†’ Ambiguous â†’ 20% lower success</li>
                </ul>
                <p>Example:</p>
                <ul>
                  <li>"Close the toilet" â†’ 0% success</li>
                  <li>"Close the white lid of the toilet" â†’ 100% success</li>
                </ul>
              </div>
            </div>
            
        <!-- Quirk 4 -->
        <h4 class="title is-5">Quirk 4: What will pi0 do if there is no specific language goal?</h4>
        <div class="content has-text-justified">
          <p>
            With no language goal, the robot will try to pick up the most familiar object within dataset based on RGB image.
          </p>
          
          <p>Examples:</p>
          <ul>
            <li>Pick up the mark pen and move back and forth</li>
            <li>Reach the block back and forth</li>
          </ul>
          
          <p>
            Here, mark pen takes 16.67% of the DROID dataset. Fine tuning on DROID may increase the probability of associating image with action to pick-and-place it.
          </p>
          
          <h5 class="title is-6">Why This Matters</h5>
          
          <p>
            These quirks highlight critical limitations in Pi0's <strong>spatial reasoning</strong> and <strong>language grounding</strong>, emphasizing the need for:
          </p>
          
          <ol>
            <li><strong>Expanded Articulation Datasets</strong>: Covering closing actions and OOD objects.</li>
            <li><strong>Language Model Upgrades</strong>: Integrating LLMs for better instruction parsing.</li>
            <li><strong>Force Feedback Integration</strong>: Enabling tactile-based correction during manipulation.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section> 

<!-- Robot & Model Setup Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">3. Robot & Model Setup</h2>

        <div class="content has-text-justified">
          <p>The following are details of our experiment set up.</p>
        </div>
        
        <h3 class="title is-4">3.1 Hardware:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Franka Research 3 Arm</strong>: 7-DOF force-sensitive robot with a 3 kg payload.</li>
            <li><strong>Robotiq 2F-85 gripper:</strong> two-finger gripper with 5mm stroke and adjustable force control.</li>
            <li><strong>Cameras</strong>:
              <ul>
                <li><strong>Side-view:</strong> ZED 2 stereo camera for global scene understanding</li>
                <li><strong>Wrist-mounted:</strong> ZED Mini for close-range object manipulation</li>
                <li><strong>Perception Mode:</strong> <strong>Pure RGB</strong> (no depth calibration)</li>
              </ul>
            </li>
          </ul>
          
          <figure class="image">
            <img src="static/images/robot.png" alt="Robot setup">
          </figure>
        </div>
        
        <h3 class="title is-4">3.2 Computing</h3>
        <div class="content has-text-justified">
          <p><strong>GPU Server:</strong></p>
          <ul>
            <li><strong>GPUs:</strong> 1Ã— NVIDIA RTX A6000 (48GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.3</li>
            <li><strong>Usage:</strong> VLA model inference.</li>
          </ul>
          
          <p><strong>Workstation</strong></p>
          <ul>
            <li><strong>GPU:</strong> NVIDIA GeForce RTX 3080 (16GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.6</li>
            <li><strong>Usage:</strong> DROID low level control.</li>
          </ul>
        </div>
        
        <h3 class="title is-4">3.3 Pi0-FAST-DROID:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Vision-Language Model</strong>: <em>Paligemma 3B</em> for spatial and semantic understanding.</li>
            <li><strong>Action Expert</strong>: Build upon <em>Transfusion</em> + <em>Playground V3</em> for high-frequency control.</li>
            <li><strong>Training Data</strong>: Pretrained on Ï€ cross-embodiment robot dataset & Open X-Embodiment, fine tuned on DROID dataset.</li>
          </ul>
          <!-- TODO: add as left and right -->
          <figure class="image">
            <img src="static/images/pi0_architecture.jpg" alt="Pi0 Architecture">
          </figure>
          <figure class="image">
            <img src="static/images/dataset.jpg" alt="Large scale robotics data for pretraining">
            <figcaption>Dataset used for pretraining pi0</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section> 

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results & Insights</h2>

        <!-- Overall Results -->
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="box">
              <h3 class="title is-4">Overall Performance</h3>
              <div class="columns">
                <div class="column is-half">
                  <figure class="image">
                    <div class="columns">
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-scatter.png" alt="Scatter plot showing performance distribution">
                      </div>
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-bar.png" alt="Chart showing overall performance statistics">
                      </div>
                    </div>
                    <figcaption>Performance across 240+ trials (52% average progress)</figcaption>
                  </figure>
                </div>
                <div class="column is-half">
                  <div class="content">
                    <p>Across our 240+ test trials, Pi0 achieved varying degrees of success:</p>
                    <ul>
                      <li><strong>Complete Success:</strong> 38%</li>
                      <li><strong>Partial Success:</strong> 28%</li>
                      <li><strong>Complete Failure:</strong> 34%</li>
                    </ul>
                    <p>We observed that performance varied significantly based on task type, environmental conditions, and most importantly, the phrasing of instructions.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Task Specific Results -->
        <h3 class="title is-4">Task-Specific Performance</h3>
        
        <!-- Pick and Place -->
        <div class="box">
          <h4 class="title is-5">Pick-and-Place (44% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Familiar objects (e.g., pineapple toy, markers) - 90% success</li>
                  <li>Clear spatial targets ("in the pink bowl") - 85% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Large objects (black cube) - 35% success</li>
                  <li>Vague targets ("inside the bowl") - 40% success</li>
                  <li>Multi-object tasks ("Put all objects into the basket") - 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <figure class="image">
                <img src="static/images/pick_and_place_chart.jpg" alt="Chart showing pick and place performance">
              </figure>
            </div>
          </div>
        </div>
        
        <!-- Human Interaction -->
        <div class="box">
          <h4 class="title is-5">Human Interaction (53.5% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Object handovers - 46.67% success</li>
                  <li>Following a moving human - 65% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Precision interactions ("Shake hands") - 30% success</li>
                  <li>Recovery after interruption - 40% success</li>
                </ul>
              </div>
            </div>
      
            <div class="column is-4">
              <div class="columns is-multiline">
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\success\human_3_Pick up the pineapple and give it to the programmer.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Give the pineapple to the programmer"</p>
                </div>
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\failure\human_7_Pick up the whiteboard eraser and give it to the programmer.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Give the whiteboard eraser to the programmer"</p>
                </div>
              </div>
            </div>

          </div>
        </div>
        

        <!-- Coffee Machine -->
        <div class="box">
          <h4 class="title is-5">Coffee Machine (8.00% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Capsule Coffee Machine:</strong></p>
                <ul>
                  <li>Close the capsule lid of coffee machine - 50% success</li>
                  <li>Pick up the capsule from the coffee machine - 0% success</li>
                  <li>Place the capsule into the coffee machine - 0% success</li>

                </ul>
                <p><strong>Espresso Coffee Machine:</strong></p>
                <ul>
                  <li>Pick up the coffee portafilter - 0% success</li>
                  <li>Pour the coffee into the cup - 0% success</li>
                  <li>Pick up the silver milk frothing pitcher- 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <div class="columns">
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/failure/coffee_failure.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Place the capsule into the coffee machine"</p>
                </div>
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/right_Close_the_capsule_lid_of_the_coffee_machine.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Close the capsule lid of the coffee machine"</p>
                </div>
              </div>
            </div>
          </div>
        </div>
        
            <div class="column is-half">
              <figure class="image">
                <img src="static/images/instruction_word_cloud.jpg" alt="Word cloud of instructions used in testing">
                <figcaption>Instruction word cloud used in our evaluations</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Future Directions -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">The Bigger Picture</h2>

        <div class="content">
          <h3 class="title is-4">Why Pi0 Matters</h3>
          
          <div class="columns">
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Generalization</h4>
                <p>Pi0 shows decent zero-shot performance across a wide range of tasks. For centuries, humans have had a strong desire to build a generalist agent system that could work in the wild, going out of the robotics lab and assisting our daily life. With the rapid development of VLAs model, the existence of Pi0 marks a milestone for robotics research.</p>
              </div>
            </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Scalability</h4>
                <p>FAST tokenizer enables 5x faster training, which is an impressive milestone and turning point for VLA research. The effective utilization of data implies that what matters to robots is not only the scaling size of dataset, but also the representation's expressiveness.</p>
              </div>
            </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Open Source Impact</h4>
                <p>The open-sourcing of Pi0 is very intuitive and warm-hearted for the whole robotics industry. The impact not only lies in that every lab gets a very powerful manipulation baseline, but it will also push and extend the horizon of research in manipulation problems.</p>
              </div>
            </div>
          </div>
          
          <h3 class="title is-4">Future Directions</h3>
          
          <div class="columns">
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Better Language Grounding</h4>
                <p>As discussed in their new work "Hi Robot", VLA's language understanding ability could be enhanced via a two-tiered inference framework. However, as we saw in our examples, the 3B VLM backbone may still suffer from instruction following & generalization across language axes.</p>
              </div>
            </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Force Feedback Integration</h4>
                <p>Current models rely primarily on visual input. Incorporating force feedback could help with tasks requiring delicate pressure control and improve interaction with varied surfaces.</p>
              </div>
            </div>
            
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Memory Modules</h4>
                <p>Currently, Pi0's autoregressive paradigm has achieved great success, but lacking history/memory capabilities is a significant limitation. Future models will likely incorporate explicit memory mechanisms to maintain task state across interruptions.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conclusion</h2>

        <div class="content has-text-justified">
          <p>Pi0 represents a promising step toward generalist robots, but significant challenges remain. Instruction following and fine-grained tasks still pose considerable difficulties. As foundation models evolve and incorporate more modalities and memory capabilities, we're optimistic about the futureâ€”soon, every robotics lab might have a Pi0-like baseline!</p>
          
          <div class="buttons is-centered mt-5">
            <a href="#" class="button is-link is-medium">
              <span class="icon">
                <i class="fas fa-table"></i>
              </span>
              <span>Explore Full Results</span>
            </a>
            
            <a href="#" class="button is-success is-medium">
              <span class="icon">
                <i class="fas fa-video"></i>
              </span>
              <span>Watch Full Videos</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Citation TODO change site-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <p>If you find this evaluation useful for your research, please consider citing our repository:</p>
          <pre><code>@misc{pi0-experiment-wild,
  author = {PAL Research Group},
  title = {Pi0 Experiment in the Wild},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/penn-pal-lab/Pi0-Experiment-in-the-Wild}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Conclusion Image -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <figure class="image">
            <img src="static/images/robot_workspace.jpg" alt="Robot Workspace">
            <figcaption>Our robot workspace setup used for evaluating Pi0.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>Pi0 Evaluation</strong> by <a href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">PAL Research Group</a>, GRASP Lab, University of Pennsylvania.
      </p>
      
      <p>
        The website template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
