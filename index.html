<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies">
  <meta name="keywords" content="pi0, Robotics, VLA, Vision-Language-Action, PAL, GRASP Lab">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Evaluating pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://grasp.upenn.edu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://grasp.upenn.edu/">
            GRASP Lab
          </a>
          <a class="navbar-item" href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">
            PAL Research Group
          </a>
          <a class="navbar-item" href="https://www.cis.upenn.edu/~kostas/">
            Daniilidis Research Group
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Evaluating pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://everloom-129.github.io/">Jie Wang</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.grasp.upenn.edu/people/matthew-leonard/">Matthew Leonard</a>,</span>
            <span class="author-block">
              <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,</span>
            <span class="author-block">
              <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>,</span>
            <span class="author-block">
              <a href="https://edwardshu.com/">Edward S. Hu</a> 
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">GRASP Lab, University of Pennsylvania</span>
          </div>
          <!-- TODO ADD formal note -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">Published:June 14, 2025</span> 
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">*Corresponding author: tonyw3@seas.upenn.edu</span> 
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper links -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2501.09747" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>pi0-FAST Paper</span>
                </a>
              </span> -->
              <!-- Paper links -->
              <span class="link-block">
                <a href="https://www.physicalintelligence.company/blog" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>PI Research</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://droid-dataset.github.io/droid/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>DROID Dataset</span>
                </a>
              </span>

              <!-- GitHub link -->
              <span class="link-block">
                <a href="https://github.com/Physical-Intelligence/openpi" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Openpi Github</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Citation Box -->
<section class="section" style="padding-top:1rem; padding-bottom:1rem;">
  <div class="container is-max-desktop">
    <div class="box has-background-light is-size-7">
      <p class="has-text-weight-semibold mb-1">Please Cite&nbsp;this&nbsp;work if you find it helpful!</p>
      <pre style="white-space:pre-wrap;font-size:0.65rem;line-height:1.2em;background:transparent;border:0;padding:0;">J.&nbsp;Wang*, M.&nbsp;Leonard, K.&nbsp;Daniilidis, D.&nbsp;Jayaraman, &amp; E.&nbsp;S.&nbsp;Hu. (2025). <em>Evaluating π<sub>0</sub> in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies.</em><a href="https://penn-pal-lab.github.io/pi0-Experiment-in-the-Wild" target="_blank" rel="noopener">Blog&nbsp;</a></pre>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="static/videos/success/success_novel_fold_newspaper.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        "Fold up the newspaper"
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Introduction</h2> -->
        <div class="content has-text-justified">
          <p>
            pi0 is a state-of-the-art vision-language-action (VLA) model designed for general-purpose robotic manipulation, trained on internet-scale data and the Open X-Embodiment dataset. While pi0 promises versatility across manipulation tasks, we ask: how does it perform when released in the wild?
          </p>
          <p>
            We subject pi0 to "vibe checks" through unstructured real world experimentation, just like how users rank LLMs by chatting about arbitrary topics in <a href="https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard">Chatbot Arena</a>. Users interact with the policies on a whim - improvising tasks, altering camera angles, rearranging objects, and probing edge cases. By embracing the high entropy of ad-hoc human testing, we stress-test the policy's generalization capabilities, while discovering interesting phenomena that structured evaluations may miss.
          </p>
          <p>
            Critically, vibe-based evaluation only becomes trustworthy when people can easily try out and verify the models themselves. Importantly, Pi0's ease of deployment — like open-source LLMs — makes such evaluations accessible, with no special instrumentation needed. We summarize our discoveries over 300+ trials of pi0 below, exploring its capabilities, quirks, and its implications for the future of robot learning. We hope that it inspires others to try out pi0 for themselves!
            <!-- We found the setup of pi0 to be very close to the experience of using open-source LLMs today - download the checkpoint and deploy, with no additional environment-specific instrumentation required (e.g. camera calibration, controller tuning, demo collection, etc.).  -->
          </p>
          <p>
            <strong>Key words: Evaluation, pi0, VLAs, Manipulation, Robot Learning, Imitation Learning. </strong> 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Box -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="box has-background-light">
          <h3 class="title is-4 has-text-centered">TL;DR</h3>
          <div class="content">
            <ul class="is-size-5">
              <li><strong>Overall Success:</strong> 42% average task completion across 300+ diverse trials</li>
              <li><strong>Key Finding:</strong> pi0 demonstrates impressive vision-language understanding but struggles with spatial reasoning and precise manipulation</li>
              <li><strong>Bottom Line:</strong> pi0 can perform well in-the-wild, but level of success is dependent on prompt engineering and task familiarity.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Images Section -->
<section class="hero">
  <div class="container" style="width: 50%; max-width: 50%;">
    <div class="hero-body" style="padding: 1.5rem 1.5rem;">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <!-- First row - three images side by side -->
            <div class="columns is-vcentered is-gapless" style="margin-bottom: 1rem;">
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/lab_setup.jpg" alt="GRASP Lab Setup" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
            </div>
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/robot_workspace.jpg" alt="Robot Workspace" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
            </div>
              <div class="column is-one-third">
                <figure class="image" style="margin: 0;">
                  <img src="static/images/lab_setup_2.jpg" alt="Levine Setup 2" style="object-fit: cover; height: 200px; width: 100%;">
                </figure>
              </div>
            </div>
            
            <!-- Second row - manipulation scene (full width) -->
            <div class="columns is-centered" style="margin-top: 0.5rem;">
              <div class="column is-12">
                <figure class="image">
                  <img src="static/images/wild-newsppaper.png" alt="Three Camera View" style="width: 100%;">
                </figure>
              </div>
            </div>
            
            <p class="has-text-centered" style="margin-top: 1rem;">
              Our evaluation setup at GRASP Lab, showing the robot workspace, test objects, and typical manipulation scenes.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Key Findings Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">2. Key Findings</h2>

        <!-- Hero Images -->
        <div class="columns is-centered">
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-scatter.png" alt="Scatter plot showing performance distribution">
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-bar.png" alt="Bar plot showing performance distribution">
            </figure>
          </div>
        </div>
        <p class="has-text-centered" style="margin-top: 0rem;">
          <strong>Left</strong>: relationship between score and number of trials; <strong>Right</strong> the progress rate on each task.<br> <br>
        </p>
        <!-- Hero Images End -->

        <div class="content has-text-justified">
          <p>
            Through experiments across diverse manipulation tasks in the <a href="https://facilities.upenn.edu/maps/locations/levine-hall-melvin-and-claire-weiss-tech-house">Levine Hall</a> 403 & 457 at Penn Engineering School, we observed a wide range of behaviors—remarkable performances, confusing failures, and many quirks. By testing pi0 in varied environments (e.g., cluttered tables, articulated cabinets, human-involved settings), we have derived three critical insights:
          </p>

          <ol>
            <li>
              <strong>pi0 Works (Mostly)</strong>: It achieves <strong>42.3% average progress</strong> across diverse, hard tasks, even in unseen environments and objects. However, it also fails in some seemingly simple tasks.
            </li>
            <li>
              <strong>Prompt Engineering Matters</strong>: While pi0 is good at vision-action grounding, its performance drops <strong>20~100%</strong> based on instruction phrasing. You need to carefully prompt it to make it work in some condition.
            </li>
            <li>
              <strong>Unexpected Quirks</strong>: pi0 can recover from failures, and handle moving humans in the scene, but struggles with <strong>mid-task freezing, collision avoidance, and fine-grained manipulation etc.</strong>
            </li>
          </ol>
          
          <p>
            Below, we unpack each finding with video examples and analysis.
          </p>

          <h3 class="title is-4">Evaluation: Progress Score through Vibe Testing</h3>
          
          <p>
            Evaluating large generalist robot policies like <strong>pi0</strong> remains an open challenge. Unlike classical robotics benchmarks, these models operate across diverse tasks and environments, often in open-ended settings with no binary notion of success.
          </p>

          <p>
            In our blog, we adopt a <strong>progress score</strong> as the main evaluation signal, which is <strong>loosely inspired by the "vibe testing" rubric</strong> proposed in the <a href="https://arxiv.org/abs/2403.04132">ChatbotArena(Chiang, etc, 2024)</a>, (<a href="https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation">Lambert 2024</a>). This method has also been used in recent works such as the pi0 and pi0-FAST papers. During evaluation, we assign a continuous score from 0 to 100 to each policy rollout, reflecting the <strong>fraction of the commanded task successfully completed</strong>. For instance, a <strong>newspaper-folding</strong> task might score:
          </p>

          <div class="columns"> 
            <div class="column is-half"> 
              <ul>
                <li><strong>25%</strong>: if the robot approached and grasp a side.</li>
                <li><strong>50%</strong>: if it fold up the newspaper in correct articulation direction.</li>
                <li><strong>75%</strong>: if the newspaper is partially fold up.</li>
                <li><strong>100%</strong>: if the newspaper is fold neatly in half.</li>
              </ul>

              <p>
                These scores are inherently subjective and noisy, but practical for generalist policy evaluation: we try to remain <strong>consistent across trials</strong>, and use them as <strong>noisy but informative approximations of task completion</strong>.
              </p>
            </div>

            <div class="column is-half">  
              <img src="static/images/worldcloud.png" alt="Word cloud visualization" style="width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto;">
              <figcaption style="text-align: center; margin-top: 0.5em;">Word cloud of task instructions</figcaption>
            </div> 
          </div>

          <p>
            This scalar scoring allows us to quickly assess success modes, failure modes, and emerging capabilities across the experiments. With the development of robotics evaluation, we believe more standardized, benchmark-driven evaluation methods for generalist policies will emerge. For now, <strong>vibe-informed progress scores</strong> are a reasonable and transparent compromise.
          </p>

          <p>
            For more details, see <a href="#appendix">Appendix B: Results</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Disclaimer Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="box has-background-warning-light">
          <h3 class="title is-4">Disclaimer</h3>
          <div class="content has-text-justified">
            <p>
              Our evaluations were conducted using the <strong>π₀-FAST-DROID</strong> model, specifically on <strong>DROID robot setups</strong>. While the model supports prompt-based control, its <strong><i>"LLM-like-download-and-prompt"</i></strong> functionality is currently limited to <strong>embodiments similar to those seen during training</strong>—particularly setups included in the <strong>Distributed RObot Interaction Dataset (DROID)</strong>.
            </p>
            <p> 
              The evaluation is conducted from Feb. 22 to March. 22, with a focus on limited environment and tasks type. Please check the Appendix section for more details on results. 
            </p>
            <p>
              PAL Research Group also contributed data to the DROID dataset in late 2023, making our setup more in-distribution. However, our setup has changed significantly since late 2023 - signficant environmental changes like camera positions, robot height and calibration, new objects, lighting, etc. All results, observations, and analyses presented in this blog are based solely on  <strong>our own rollouts</strong>, and <strong>do not reflect any official performance</strong> guarantees from the model authors or dataset maintainers.
            </p>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Success Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Success Cases</h2>

        <!-- Success videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Pick and Place</h3>
                  <div>
                    <video id="success-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_pickplace_fish_to_box.mp4" type="video/mp4">
      </video>
    </div>
                  <p class="is-size-6">"Place the yellow fish into the purple box"</p>
                  <p class="is-size-6">Precise placement of camouflage fish into the box</p>
  </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Articulation</h3>
                  <div>
                    <video id="success-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_articulation_open_drawer.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Open the drawer"</p>
                  <p class="is-size-6">Opening drawer with multiple pull to make sure fully opened</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Human Robot Interaction</h3>
                  <div>
                    <video id="success-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_hri_handover_pineapple.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Hand the pineapple to the programmer"</p>
                  <p class="is-size-6">Safe object handover to programmer, even there is wire occlusion from side view</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Dexterity</h3>
                  <div>
                    <video id="success-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_dexterity_pour_water.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pour water from the silver cup to the pink bowl"</p>
                  <p class="is-size-6">Pour real water from Latte art vat into the target bowl</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Multi-step Task</h3>
                  <div>
                    <video id="success-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/success_multistep_fill_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Pick up all the objects into the basket"</p>
                  <p class="is-size-6">Sequential bagging all the toys into basket</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Novel Objects</h3>
                  <div>
                    <video id="success-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static/videos/success/right_Close_the_capsule_lid_of_the_coffee_machine_left.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6">"Close the capsule lid of the coffee machine"</p>
                  <p class="is-size-6">Handling previously unseen device</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Remarkable Performance Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">3. Remarkable Performance of PI0</h2>

        <!-- Vision-Language Understanding -->
        <h3 class="title is-4">3.1 Robust Vision-Language Understanding in Complex Scenes</h3>
        <div class="content has-text-justified">
          <p>
            Powered by <strong>PaliGemma</strong> (Google DeepMind's 3B VLM) as its vision encoder (<a href="https://arxiv.org/abs/2407.07726">Beyer etc, 2024</a>), pi0 demonstrates robust scene comprehension and adaptability. Despite relying solely on uncalibrated monocular RGB inputs (224x224 pixels after compression), it can handle very challenging objects and environments like transparent, camouflaged and novel items.            
          </p>
          
          <p><strong>1. It can grasp transparent objects</strong></p>
          
          <p>
            PI0 is capable of identifying and manipulating transparent objects, as shown below. It picks up the bottle with a stable grasp, aligns it to the small cup, and precisely drops it in. Many traditional grasp detection techniques require an accurate 2D or 3D reconstruction of the scene, and transparent objects can cause issues in reconstruction accuracy. This makes it all the more impressive that the model can detect transparent objects solely from uncalibrated, mono-RGB views from the wrist camera and one side view camera.
          </p>
          
          <div class="columns is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\Success_Dropping_Into_Cup.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the white cup."</p>
        </div>
      </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\transparent_bottle_place.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the bowl."</p>
        </div>
      </div>
          </div>

          <p><strong>2. It can grasp an object even when it is camouflaged into a colorful background</strong></p>
          
          <p>
            PI0 can identify the 'yellow fish' here even when it is placed on top of a colorful board game. This object has an unusual and difficult shape, and it blends in well with the background, but pi0 detects well with its position, and its shape, well enough to grasp it up.
          </p>
          
    <div class="columns is-centered">
            <div class="column is-half"> 
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/success/camflage_yellow_fish.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the red box"</p>
          </div>
        </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_pickplace_fish_to_box.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the fish into the purple box"</p>
      </div>
    </div>
  </div>
          
          <p><strong>3. It is robust to human activity in the input</strong></p>
          
          <p>
            During evaluation, there were many times where the side-view camera captured humans moving around in the background. However, pi0 can always focus on its task, keeping the robotic arm's movements focused on object manipulation.
          </p>
          
          <p>
            We believe there are two reasons for pi0's robustness to human movement. First, the pre-trained VLM backbone of pi0 is trained on human-involved images (<a href="https://aclanthology.org/P18-1238/">Sharma et al., 2018</a>, <a href="https://aclanthology.org/2022.naacl-main.142/">Changpinyo et al., 2022a</a>, <a href="https://storage.googleapis.com/openimages/web/factsfigures_v7.html">Kuznetsova et al. 2020</a>), so humans are in-distribution. Next, as our occlusion experiments in <strong>Section 2.3.1</strong>, the policy seems to prioritize the wrist camera's images during pick-and-place tasks, so distractors in the side-view camera seem to minimally affect the policy.
          </p>
          
          <p>
            Here are two side-view videos involving humans in the scene. Please refer to <strong>Appendix B.6</strong> for more experiments on <strong>human-robot interaction</strong>.
          </p>

          <p><em>(All videos featuring humans were uploaded with permission from the individuals involved.)</em></p>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_initial_PI0_pineapple.mp4" type="video/mp4">
      </video>
                <p class="is-size-6 has-text-centered">"Pick the pineapple and place it into the basket"</p>
    </div>
  </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_hand_the_pineapple_to_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to the outstretched hand"</p>
              </div>
            </div>
          </div>
            

          <p>
            There's plenty of work in the past in CV, Robotics that does transparent object detection and manipulation. But the nice part here is that we have a data-driven system that does it, without any special logic or care for transparent objects.
          </p>
          <p class="box has-background-grey-lighter has-text-centered is-italic">
            "pi0's ability to handle transparency, clutter and distractors hints at a future where robots see the world as humans do—through semantics, not just pixels."
          </p>
          
        </div>
        
        <!-- Behavioral Structure: pi0 is an FSM -->
        <h3 class="title is-4">3.2 Behavioral Sequencing in pi0</h3>
        <div class="content has-text-justified">
          <p>
            Through our experiments, we observed that pi0 exhibits intuitive patterns in its behavior across a wide range of manipulation tasks. Despite being an autoregressive model without explicit memory or state, pi0 often transitions through intuitive behavioral phases such as:
          </p>
          
          <p style="text-align: center;">
            <strong>Search → Reach → Grasp → Transfer → Release → Reset → Idle</strong>
          </p>

          <p>
            What's remarkable is that this structure <strong>emerges naturally from the data</strong> rather than being explicitly modeled — suggesting that pi0 learns consistent <strong>task execution priors</strong> across environments. For instance, even when pi0 is unfamiliar with an object or task, it often <strong>proactively explores</strong> near affordance-rich areas using its wrist camera to decide whether to grasp or not.
          </p>

          <p>
            In certain trials, we also observed <strong>reset-like behaviors</strong>: if pi0 perceives the task as complete (e.g., after placing an item into a bowl), it may <strong>return to its home configuration and idle</strong>. While this often indicates a well-formed task boundary, it can also lead to <strong>early stopping /freeze issues</strong>, especially in multi-object scenes — see <strong>Section 4.1</strong> for analysis of early stopping failure cases.
          </p>

          <p>
            <em>While this sequencing might suggest that pi0 has learned an internal understanding of the task, we caution against such framing. These patterns may reflect properties of the data distribution (e.g., Markovian, short-horizon tasks), rather than indicating the policy has acquired explicit task inference or memory.</em>
          </p>

          <!-- Existing video grid -->
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_remove_the_pink_bowl_from_the_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Remove the pink bowl from the tray"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\left_stack_the_wooden_blocks.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Stack the wooden blocks"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\success_fold_the_cloth_from_left_to_right.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold the cloth from left to right"</p>
              </div>
            </div>
          </div>
          
          <!-- <p class="box has-background-grey-lighter has-text-centered is-italic">
            "pi0's FSM pattern indicates it is a strong meta learning algorithm being able to generalize across environments with pretrained data."
          </p> -->
        </div>
        
        <!-- Failure Recovery -->
        <!-- <h3 class="title is-4">3.3 Failure recovery and generalization</h3>
        <div class="content has-text-justified">
          <p>
            One key difference between the pi0 and other open source VLA models are: it can work in the wild, without online data fine-tuning, without calibration, without much position tuning on the policy, we can let it run on our franka robot directly.
          </p>
          
          <p>
            More interestingly, the VLA behaves similar to diffusion policy, as shown below, it could recover from failures and retry tasks that initially failed.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\3.3place_the_pen_into_pen_holder.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the pen into pen holder"</p>
              </div>
              <div>
            </div>
     
          </div>
          </div>

          <div class="columns is-multiline is-centered">
            <div class="column is-full">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\success\3.3_fold_up_the_newspaper_from_right_side.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Fold up the newspaper from left side"</p>
              </div>
            </div>
          </div>
          


        </div> -->
      </div>

      
    </div>
  </div>
</section>

<!-- Failiure Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Failure Cases</h2>
        <p class="has-text-justified" style="color: #666; font-style: italic;">
          pi0 demonstrates very impressive robustness across different lights, locations and tasks. However, it does have failure cases:
        </p>
        <!-- Failure videos -->
        <div class="columns is-centered">
          <!-- Video Grid layout - 2x3 grid for 6 videos -->
          <div class="column">
            <div class="columns is-multiline">
              <!-- Video 1 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Objects</h3>
                  <div>
                    <video id="failure-video-1" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\4_failure_pour_the_water_from_teapot_into_bowl.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour water from teapot into bowl"</i></p>
                  <p class="is-size-6">Cannot manipulate a new glass teapot (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 2 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">OOD Background</h3>
                  <div>
                    <video id="failure-video-2" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\upenn_pick_block.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pick the black box on the white box"</i></p>
                  <p class="is-size-6">Can not handle well with unseen background (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 3 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Task Misunderstanding</h3>
                  <div>
                    <video id="failure-video-3" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_fish_into_the_basket.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"place the yellow fish into the basket"</i></p>
                  <p class="is-size-6">Pick up the wrong object in cluttered scene (25% success rate)</p>
                </div>
              </div>
              
              <!-- Video 4 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Spatial Reasoning</h3>
                  <div>
                    <video id="failure-video-4" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\right_place_the_can_into_the_tray.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Place the can into the tray"</i></p>
                  <p class="is-size-6">Misjudges object position relative to container (30% success rate)</p>
                </div>
              </div>
              
              <!-- Video 5 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">General Articulation</h3>
                  <div>
                    <video id="failure-video-5" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\video_close_the_right_cabinet_door.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Close the right cabinet door"</i></p>
                  <p class="is-size-6">Fails to open toy kitchen cabinet on the table (0% success rate)</p>
                </div>
              </div>
              
              <!-- Video 6 -->
              <div class="column is-one-third">
                <div class="content">
                  <h3 class="title is-5">Coffee Making</h3>
                  <div>
                    <video id="failure-video-6" autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                      <source src="static\videos\failure\coffee_pour_coffee_bean_into_grinder.mp4" type="video/mp4">
                    </video>
                  </div>
                  <p class="is-size-6"><i>"Pour coffee bean into the grinder"</i></p>
                  <p class="is-size-6">Cannot work with espresso machine (0% success rate)</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Problems with PI0 Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">4. Problem with PI0</h2>

        <!-- Early stopping problem -->
        <h3 class="title is-4">4.1 Early stopping</h3>
        <div class="content has-text-justified">
          <p>
            One common failure case is that policy may <strong>freeze unexpectedly during execution</strong>, leaving tasks incomplete until human intervention. 
            This behavior comes from two interrelated factors: semantic ambiguity and autoregressive action decoding limitations.
          </p>


          <!-- 3 video examples of early stopping -->
          <div class="columns is-multiline is-centered">
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_hand_the_pineapple_to_human.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Hand the pineapple to human"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_open the drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Open the drawer"</p>
              </div>
            </div>
            <div class="column is-one-third">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\problems\4.1_pour coffee bean into the coffee grinder.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Pour coffee bean into the coffee grinder"</p>
              </div>
            </div>
          </div>


          
          <h4 class="title is-5">Possible Causes</h4>
          
          <p><strong>1. VLA doesn't understand the instruction</strong></p>
          <p>
            pi0 lacks LLM-like commonsense reasoning to infer unfamiliar object categories. When it does not understand a command, it gets stuck. 
            In some experiments, we found that some objects / instructions are out of distribution(OOD), causing early stopping.
          </p>
          
          
          <p><strong>2. VLAs are based on Markov Assumption, but World is not MDP</strong></p>
          <p> 
            pi0 is a <strong>memory-less</strong> policy, meaning its next action depends only on the current observation—not past actions. When tasks involve <strong>subtle temporal dependencies</strong>, this behavior can cause <strong>valid-looking but incomplete behavior</strong>.

          </p>
          <ul>
            <li><strong>Case</strong>: "Open the drawer" → stops after grasping the handle.</li>
            <li><strong>Behavior</strong>: pi0 grasps the handle, then does nothing. The output actions are very small. </li>
          </ul>
          <p>
            VLA assumes task completion after the initial grasp, unaware that <strong>drawer articulation is a multi-step process</strong>. pi0 always picks the most likely (mode) action. It is the failure you'd expect when the Markovian property does not hold for a given task. If the majority of training data in a given context (e.g. robot in home position, grasped drawer handle) corresponds to no motion, pi0 will always output "zero joint velocity"—and get stuck.
          </p>
          
          

        
          <div class="columns">
            <div class="column is-half">
              <p><strong>3. Token Decoding Edge Cases</strong></p>
              <p>
                During inference, pi0 will throw out this error: <code>Error decoding tokens: cannot reshape array of size 79 into shape (8)</code>
              </p>
              <p>
                According to our discussion on <a href="https://github.com/Physical-Intelligence/openpi/issues/373">Github Issue#373</a>, sometimes the policy decoded mis-shaped actions occasionally during inference. 
                In official implementation, <strong>pi0-fast-droid</strong> defaults to "no-motion" in these cases. Since as the robot continues querying the policy, 
                the error gets skipped on subsequent queries, allowing the robot to quickly recover and continue decoding correctly-shaped outputs.
              </p>
              <p><em style="color: red;">Warning: Don't kill PI0 Inference Server when early stops, be careful the robot may continue moving before server restarts!</em></p>
            </div>
            <div class="column is-half">
              <div>
                <img src="static/images/eval_Articulation_open the upper drawer_visualization.png" alt="Drawer articulation visualization" width="100%" style="border-radius: 10px;">
                <p class="is-size-6 has-text-centered">A visualization of Robot Joint and Gripper state of early stops</p>
              </div>
            </div>
          </div>
        
        </div> <!-- End of 4.1 -->





        <!-- Imprecise spatial reasoning -->
      <h3 class="title is-4">4.2 Imprecise spatial reasoning</h3>
        <div class="content has-text-justified">
          <p>
            pi0 often struggles with spatial reasoning over height. For example, when asked to pick up an object and place it into a container, the policy cannot lift the object high enough to clear the height of the container. This suggests the drawback of <strong>vision-based policies</strong>: it does not have a metrically accurate method to know the distance between gripper and surrounding environment. 
          </p>

          <div class="columns">
            <div class="column is-half">
              <p>
              As shown here, the robot seems to think the gripper is high enough, and so it pushes the destination container. If the policy are provided with <strong>Camera sensor</strong>, then it should be able to accurately estimate the size of the object that it holds, and that of the gap between the height of the object and the bowl, and understand that it must increase the gap, it could complete the task successfully.
              </p>
            </div>

            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/Pushing_Bowl_Fail.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the plastic bottle into the pink bowl" - pi0 fails to lift high enough and always collide</p>
              </div>
            </div>
        </div>

        <div class="columns">
          <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.2_touch_the_index_finger_of_the_outstretched_hand.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Touch the index finger of outstretched hand" - pi0 correctly touch, but with gripper force</p>
              </div>
            </div>
            <div class="column is-half">
              <p>
                We also tried to prompt pi0 to raise the gripper higher. Like "raise the bottle high enough / up 10cm to avoid collision...". But it didn't learn on them.
                When pi0 is asked to operate with an articulation object, it becomes even harder for it to estimate the distance from the side view camera, causing frequent collisions. This is particularly worth noting when the robot is intersecting with humans. As it doesn't have any safety constraint,  it will accidentally hit / grasp the user's hand, which can be hurt!. 
              </p>
            </div>
          </div>

          <p>
            What's more, when pi0 is required to manipulate a new household appliance, it will tend to collide with the device or stop during inference trials. As shown below, pi0 can not use the Coffee Machine in our lab.
          </p>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_take_out_the_cup_under_the_coffee_machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Take out the cup under the coffee machine" → collides with top of machine</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_raise the lever of coffee machine.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Raise the lever of coffee machine" → didn't understand where is lever.</p>
              </div>
            </div>
          </div>

          <p>
            One possible solution is to involve concepts like Vox map and planning constraints. Depth cameras would also be very helpful to implement collision avoidance.
          </p>
        </div>
<!-- 
      <h3 class="title is-4">4.3 Sensitivity to Environment Change</h3>
        <div class="content has-text-justified">
          <p>
            It is well-known that most robot policies are sensitive to environment change. Change the camera slightly or the initial position of the objects and the success rate can plummet. We found that pi0 displays some robustness, but performance can be dramatically improved by moving the test distribution closer to the training dataset.
          </p>

          <h5 class="title is-6">1. Orientation of objects significantly affects performance</h5>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/problems/4.3_place_the_can_into_red_tray.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">"Place the can into the red tray" - Success rate varies with can orientation</p>
              </div>
            </div>
            <div class="column is-half">
              <p>
                As shown, when the spam can is parallel to the viewer, the robot fails consistently. When perpendicular, it succeeds consistently. This orientation bias likely stems from the viewpoint distribution in the training data, where certain viewpoints were overrepresented.
              </p>
            </div>
          </div> -->

<!-- 
          <p>
            Potential solutions might involve augmenting behavior cloning with:
          </p>

          <ul>
            <li>Explicit object-centric representations to identify functional parts</li>
            <li>Adversarial training to improve robustness to environment change</li>
            <li>Incorporating physical priors about stable grasps and object affordances</li>
          </ul> -->
        </div>
    </div>
    
</section>


<!-- Quirks Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">5. Quirk: Some interesting behavior of pi0</h2>

        <h4 class="title is-5">Quirk 1: Prompt Engineering matters</h4>
        <div class="content has-text-justified">
          <p>
            pi0's performance heavily depends on instruction, leaving space for prompt engineering. Let's look at two interesting aspects:
          </p>

          <h5 class="title is-6">1.1 You need to tune your prompt carefully to operate the robot</h5>
          pi0 freezes or fails when instructions contain typos, grammatical errors, or ambiguous phrasing. Here for example, when we are trying to let pi0 manipulate the articulated objects, we may need to try multiple different prompt to find the <em>'in-distribution'</em> instructions.

          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Instruction</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><em>"Close the toilet"</em></td>
                <td>0%</td>
                <td>Wanders aimlessly, unable to localize the target.</td>
              </tr>
              <tr>
                <td><em>"Close the white lid of the toilet"</em></td>
                <td>100%</td>
                <td>Always closes the toy toilet.</td>
              </tr>
            </tbody>
          </table>
          
          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\quirk\close_the_white_lid_of_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the white lid for the toilet (Success)</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static\videos\quirk\close_the_toilet.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered">Close the toilet (Failure)</p>
              </div>
            </div>
          </div>

          <h5 class="title is-6">1.2 pi0's behavior without language goals</h5>
          
          <p>
            When given no specific language instruction, pi0 defaults to interacting with the most familiar objects from its training data:
          </p>

          <ul>
            <li>Given nonsense text like <em>"dgbfzjkfhjilawhdfkAWHDKLWHADFiQAWFHqawipfjcasklfmdc"</em>, it picks up marker pens</li>
            <li>Given <em>"xxx"</em>, it reaches for blocks repeatedly</li>
          </ul>

          <p>
            As in DROID dataset, marker pens comprise 16.67% of objects, so it probably make pi0 tend to pick it up with only vision guidance.  Default behaviors are heavily influenced by training data distribution. How to overcome this ambiguity and reject invalid instruction is still an ongoing problems. 
          </p>


        <div class="columns is-multiline is-centered">
          <div class="column is-half">
            <div>
              <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                <source src="static\videos\quirk\nonsense-pen.mp4" type="video/mp4">
              </video>
              <p class="is-size-6 has-text-centered">dgbfzjkfhjilawhdfkAWHDKLWHADFiQAWFHqawipfjcasklfmdc<br>(nonsense, always pick up pen)</p>
            </div>
          </div>
          <div class="column is-half">
            <div>
              <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                <source src="static\videos\quirk\nonsense-block.mp4" type="video/mp4">
              </video>
              <p class="is-size-6 has-text-centered">xxx<br>(nonsense, go back and forth)</p>
            </div>
          </div>
        </div>


        <h4 class="title is-5">Quirk 2: How robust is pi0 under partial observability?</h4>
        <div class="content has-text-justified">
          <p>
            One of the most frequently asked questions on pi0 is, how robust it is when its visual inputs are disrupted? We did several tests on blocking the camera and the object. Here's what we found.
          </p>

          <h5 class="title is-6">Camera Blocking Experiments</h5>
          
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task: "Pick up the pink object and place it into the bowl."</strong></li>
            <li><strong>Cameras</strong>: Side-view (primary) + wrist-mounted (secondary).</li>
            <li><strong>Blocking Scenarios</strong>: Partial/full occlusion of one or both cameras.</li>
            <li><strong></strong>Test time</strong>: 4 trials per scenario, 300 rollouts per trial.</li>
          </ul>


          <div class="column is-full-width">
            <div>
              <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                <source src="static/videos/quirk/AP_block_leftcamera_pick_up_the_purple_object.mp4" type="video/mp4">
              </video>
              <p class="is-size-6 has-text-centered"><strong>Block the left camera:</strong> pi0 can still find pink object.</p>
            </div>
          </div>

          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Blocking Type</th>
                <th>Success Rate</th>
                <th>Behavior</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>No Block</strong></td>
                <td>100%</td>
                <td>Baseline: Perfect execution. Picks correct object, then explores others.</td>
              </tr>
              <tr>
                <td><strong>Block Side Camera Mid-Trial</strong></td>
                <td>50%</td>
                <td>Relies on wrist camera to pick up the object → success rate decreases.</td>
              </tr>
              <tr>
                <td><strong>Block Wrist Camera Entirely</strong></td>
                <td>0%</td>
                <td>Frozen—no recovery.</td>
              </tr>
              <tr>
                <td><strong>Block Both Cameras Initially → Unblock Mid-Trial</strong></td>
                <td>75%</td>
                <td>Chaotic exploration → can execute after unblocking.</td>
              </tr>
            </tbody>
          </table>

          <div class="columns is-multiline is-centered">
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_0.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>No Block:</strong> 100% execution.</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_1.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Side Camera Blocked Mid-Trial:</strong> Robot is interrupted but can still complete the task.</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_2.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Wrist Camera Blocked Entirely:</strong> Frozen.</p>
              </div>
            </div>
            <div class="column is-half">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/blocking_camera_3.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Both Blocked, then Unblocked:</strong> Recovers.</p>
              </div>
            </div>
          </div>


          <h5 class="title is-6">Object Blocking Experiment</h5>
          <p><strong>Setup</strong>:</p>
          <ul>
            <li><strong>Task</strong>: "Pick up the pineapple."</li>
            <li><strong>Occlusion Levels</strong>: None (fully visible), 50% occluded, 100% occluded.</li>
            <li><strong>Test time</strong>: 12 trials per scenario, 300 rollouts per trial.</li>
          </ul>

            <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_box_pick_up_the_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>From three boxes: 66.67%</strong> Pi0 can easily find the target object from different boxes on the table.</p>
              </div>
            </div>
            <!-- <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_box_moving_pick_up_the_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>From Boxes, but moving:</strong>Switching the boxes to make test more challenging.</p>
              </div>
            </div> -->

            <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_drawer_find_pineapple_inside_drawer.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Inside Drawer: 25%</strong> 
                  <br>
                  <em>Franka's collision with the drawer makes pi0 hard to pick out the pineapple.</em>
                </p>
              </div>
            </div>
  

            <div class="column is-full-width">
              <div>
                <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                  <source src="static/videos/quirk/AP_interactive_pick_the_pineapple.mp4" type="video/mp4">
                </video>
                <p class="is-size-6 has-text-centered"><strong>Hidden under cloth: 0%</strong>
                  <br>
                  <em>pi0 is unable to explore the environment that requires interaction exploration:.</em>
                </p>
              </div>
            </div>

          
  
        </div>


        <h5 class="title is-6">Our Observations</h5>
        <ul>
          <li><strong>Active Perception</strong>:
            <ul>
              <li>When the side camera is blocked, pi0 can still complete the task with only wrist camera.</li>
              <li>In most pick and place tasks, pi0 relies on wrist camera as the side-view camera may be too far.</li>
              <li>Wrist camera plays as active perception, exploring the scene and pick up the object if it is in center-up area. </li>
            </ul>
          </li>
          <li><strong>Viewpoint Robustness</strong>:
            <ul>
              <li>pi0 can tolerate changing side-view camera position and orientation during the task</li>
              <li>Blocking then unblocking cameras triggered pi0 to continue movements (The image condition changes for the policy).</li>
            </ul>
          </li>
          <li><strong>Common Failure Modes under Partial Observability</strong>:
            <ul>
              <li>Total wrist camera blockage → robot freezes and can execute, even if the side image is available.</li>
              <li>Pi0 is memoryless, predicting the action per frame auto-regressively, so it will be able to continue executing the task if observation is available.</li>
              <li>However, the exploration behavior of pi0 is not efficient and limited to certain area. Making it hard to actively search the environment.</li>
            </ul>
          </li>
        </ul>
        
      </div>
    </div>
  </div>
</section>


<!-- Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">7. Conclusion</h2>

        <div class="content has-text-justified">
          <p>Through our evaluations, we conclude that Pi0 is a useful generalist policy, capable of doing sensible behavior in unseen manipulation tasks. For our lab, Pi0 will serve as a bedrock layer to build specific functionality on top of, as well as a baseline for comparison.  However, many challenges remain - our experiments showed that Pi0's performance is sensitive to its prompts and struggles with instruction following, fine-grained tasks and partial observability. We are optimistic that these areas will be improved by the robot learning community, and believe that generalist policies will be widely used in the future.</p>     
          <!-- <div class="buttons is-centered mt-5">
            <a href="#" class="button is-link is-medium">
              <span class="icon">
                <i class="fas fa-table"></i>
              </span>
              <span>Explore Full Results</span>
            </a>
            
            <a href="#" class="button is-success is-medium">
              <span class="icon">
                <i class="fas fa-video"></i>
              </span>
              <span>Watch Full Videos</span>
            </a> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- References -->
<section class="section" id="references">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">8. References</h2>
        <div class="content">
          <div class="csl-bib-body">
            <!-- 1. pi0 -->
            <div class="csl-entry">
              K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, et al. (2024).
              Black, o., Xia, Z., Ha, L., Kaplan, A., Huang, H., Hausman, K., Ichter, B., Fox, D., & Levine, S. (2024). 
              <em><span class="paper-title">π0: A vision-language-action flow model for general robot control.</span></em>
              <span class="paper-conference">arXiv preprint arXiv:2410.24164.</span>
              <a href="https://arxiv.org/abs/2410.24164">[Paper]</a>
              <a href="https://www.pi.website/blog/pi0">[Project Page]</a>
            </div>
            
            <!-- 2. FAST -->
            <div class="csl-entry">
              K.&nbsp;Pertsch, K.&nbsp;Stachowicz, B.&nbsp;Ichter, D.&nbsp;Driess, S.&nbsp;Nair, Q.&nbsp;Vuong, O.&nbsp;Mees, C.&nbsp;Finn, &amp; S.&nbsp;Levine. (2025).
              <span class="paper-title"><em>FAST: Efficient action tokenization for vision-language-action models.</em></span>
              <span class="paper-conference"><em>arXiv&nbsp;preprint</em> arXiv:2501.09747.</span>
              <a href="https://arxiv.org/abs/2501.09747">[Paper]</a>
              <a href="https://www.pi.website/research/fast">[Project&nbsp;Page]</a>
            </div>

            <!-- 9. OpenPI repository -->
            <div class="csl-entry">
              Physical-Intelligence. (2025).
              <span class="paper-title"><em>OpenPI.</em></span>
              <span class="paper-conference"><em>GitHub&nbsp;repository.</em></span>
              <a href="https://github.com/Physical-Intelligence/openpi">[GitHub]</a>
            </div>

            <!-- 3. DROID dataset -->
            <div class="csl-entry">
              A.&nbsp;Khazatsky, K.&nbsp;Pertsch, S.&nbsp;Nair, A.&nbsp;Balakrishna, S.&nbsp;Dasari, S.&nbsp;Karamcheti, S.&nbsp;Nasiriany, M.&nbsp;K.&nbsp;Srirama, L.&nbsp;Y.&nbsp;Chen, K.&nbsp;Ellis, et&nbsp;al. (2024).
              <span class="paper-title"><em>DROID: A large-scale in-the-wild robot manipulation dataset.</em></span>
              <span class="paper-conference"><em>arXiv&nbsp;preprint</em> arXiv:2403.12945.</span>
              <a href="https://arxiv.org/abs/2403.12945">[Paper]</a>
            </div>

            <!-- 4. Chatbot Arena -->
            <div class="csl-entry">
              W.-L.&nbsp;Chiang, L.&nbsp;Zheng, Y.&nbsp;Sheng, A.&nbsp;N.&nbsp;Angelopoulos, T.&nbsp;Li, D.&nbsp;Li, B.&nbsp;Zhu, H.&nbsp;Zhang, M.&nbsp;I.&nbsp;Jordan, J.&nbsp;E.&nbsp;Gonzalez, &amp; I.&nbsp;Stoica. (2024).
              <span class="paper-title"><em>Chatbot Arena: An open platform for evaluating LLMs by human preference.</em></span>
              <span class="paper-conference"><em>Proceedings of the 41st International Conference on Machine Learning</em> (ICML 2024).</span>
              <a href="https://arxiv.org/abs/2403.04132">[Paper]</a>
              <a href="https://chat.lmsys.org">[Platform]</a>
            </div>
            <!-- 10. Interconnects blog post -->
            <div class="csl-entry">
              N.&nbsp;Lambert. (2024).
              <span class="paper-title"><em>ChatBotArena: The peoples' LLM evaluation, the future of evaluation, the incentives of evaluation, and gpt2chatbot.</em></span>
              <span class="paper-conference"><em>Interconnects.ai&nbsp;blog.</em></span>
              <a href="https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation">[Article]</a>
            </div>
      

            <!-- 5. Conceptual Captions -->
            <div class="csl-entry">
              P.&nbsp;Sharma, N.&nbsp;Ding, S.&nbsp;Goodman, &amp; R.&nbsp;Soricut. (2018).
              <span class="paper-title"><em>Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.</em></span>
              <span class="paper-conference"><em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em> (ACL 2018), 2556 - 2565.</span>
              <a href="https://aclanthology.org/P18-1238/">[Paper]</a>
            </div>

            <!-- 6. All You May Need for VQA are Image Captions -->
            <div class="csl-entry">
              S.&nbsp;Changpinyo, D.&nbsp;Kukliansy, I.&nbsp;Szpektor, X.&nbsp;Chen, N.&nbsp;Ding, &amp; R.&nbsp;Soricut. (2022).
              <span class="paper-title"><em>All you may need for VQA are image captions.</em></span>
              <span class="paper-conference"><em>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
              Human Language Technologies</em> (NAACL 2022), 1947 - 1963.</span>
              <a href="https://aclanthology.org/2022.naacl-main.142/">[Paper]</a>
            </div>

            <!-- 7. Open Images V4 -->
            <div class="csl-entry">
              A.&nbsp;Kuznetsova, H.&nbsp;Rom, N.&nbsp;Alldrin, J.&nbsp;Uijlings, I.&nbsp;Krasin, J.&nbsp;Pont-Tuset, S.&nbsp;Kamali, S.&nbsp;Popov, M.&nbsp;Malloci, A.&nbsp;Kolesnikov, T.&nbsp;Duerig, &amp; V.&nbsp;Ferrari. (2020).
              <span class="paper-title"><em>The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale.</em></span>
              <span class="paper-conference"><em>International Journal of Computer Vision</em>, 128, 1956 - 1981.</span>
              <a href="https://doi.org/10.1007/s11263-020-01316-z">[Paper]</a>
              <a href="https://storage.googleapis.com/openimages/web/index.html">[Dataset]</a>
            </div>

            <!-- 8. PaliGemma -->
            <div class="csl-entry">
              L.&nbsp;Beyer, A.&nbsp;Steiner, A.&nbsp;S.&nbsp;Pinto, A.&nbsp;Kolesnikov, X.&nbsp;Wang, D.&nbsp;Salz, M.&nbsp;Neumann, I.&nbsp;Alabdulmohsin, M.&nbsp;Tschannen, E.&nbsp;Bugliarello, et&nbsp;al. (2024).
              <span class="paper-title"><em>PaliGemma: A versatile 3B VLM for transfer.</em></span>
              <span class="paper-conference"><em>arXiv&nbsp;preprint</em> arXiv:2407.07726.</span>
              <a href="https://arxiv.org/abs/2407.07726">[Paper]</a>
            </div>

     
      

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Acknowledgments -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">9. Acknowledgments</h2>
        <div class="content has-text-justified">
          <p>
            We are grateful to Will Liang, Hungju Wang, and Sam Wang for their assistance in setting up the π0 environment. We further thank Kaustubh Sridhar, Tianyou Wang, Ian Pedroza, Ethan Yu, Tim Song, and Yiqian Li for their help doing the experiments.
          </p>
          <p>
            We also thank Junyao Shi, Aurora Qian, Leon Kim, and Jason Ma for their insightful suggestions on evaluating a generalist manipulation policy.
          </p>
          <p>
            We extend our appreciation to Karl Pertsch from Physical Intelligence for his constructive feedback on the blog draft.
          </p>
          <p>
            This work was conducted independently within the PennPAL and Daniilidis groups at the GRASP Lab.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Citation change site-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <p>If you find this evaluation useful for your research, please consider citing our repository:</p>
          <pre><code>@misc{pi0-experiment-wild,
  author = {J.&nbsp;Wang*, M.&nbsp;Leonard, K.&nbsp;Daniilidis, D.&nbsp;Jayaraman, &amp; E.&nbsp;S.&nbsp;Hu},
  title = {Evaluating pi0 in the Wild @ GRASP Lab: Strengths, Problems, and the Future of Generalist Robot Policies},
  year = {2025},
  publisher = {GitHub},
  url = {https://penn-pal-lab.github.io/pi0-Experiment-in-the-Wild}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Conclusion Image -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <figure class="image">
            <img src="static/images/robot_workspace.jpg" alt="Robot Workspace">
            <figcaption>Our robot workspace setup used for evaluating pi0.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Robot & Model Setup Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Appendix A: Our Robot & Model Setup</h2>

        <div class="content has-text-justified">
          <p>The following are details of our experiment set up.</p>
        </div>
        
        <h3 class="title is-4">Hardware:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Franka Research 3 Arm</strong>: 7-DOF force-sensitive robot with a 3 kg payload.</li>
            <li><strong>Robotiq 2F-85 gripper:</strong> two-finger gripper with 5mm stroke and adjustable force control.</li>
            <li><strong>Cameras</strong>:
              <ul>
                <li><strong>Side-view:</strong> ZED 2 stereo camera for global scene understanding</li>
                <li><strong>Wrist-mounted:</strong> ZED Mini for close-range object manipulation</li>
                <li><strong>Perception Mode:</strong> <strong>Pure RGB</strong> (no depth calibration)</li>
              </ul>
            </li>
          </ul>
          
      
      </div>
        
        <h3 class="title is-4">Compute</h3>
        <div class="content has-text-justified">
          <p><strong>GPU Server:</strong></p>
          <ul>
            <li><strong>GPUs:</strong> 1x NVIDIA RTX A6000 (48GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.3</li>
            <li><strong>Usage:</strong> VLA model inference.</li>
          </ul>
          
          <p><strong>Workstation</strong></p>
          <ul>
            <li><strong>GPU:</strong> NVIDIA GeForce RTX 3080 (16GB VRAM)</li>
            <li><strong>CUDA Version:</strong> 12.6</li>
            <li><strong>Usage:</strong> DROID low level control.</li>
          </ul>
        </div>
        
        <h3 class="title is-4">3.3 pi0-FAST-DROID:</h3>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Vision-Language Model</strong>: <em>Paligemma 3B</em> for spatial and semantic understanding.</li>
            <li><strong>FAST+</strong>: Frequency-space Action Sequence Tokenization (FAST), a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies.
            <li><strong>Training Data</strong>: Pretrained on π cross-embodiment robot dataset & Open X-Embodiment, fine tuned on DROID dataset.</li>
          </ul>
<!--           
          <figure class="image">
            <img src="static/images/dataset.jpg" alt="Large scale robotics data for pretraining">
            <figcaption>Dataset used for pretraining pi0</figcaption>
          </figure> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Appendix B: Detailed Results for Each Task</h2>

        <!-- Overall Results -->
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="box">
              <h3 class="title is-4">Overall Performance</h3>
              <div class="columns">
                <div class="column is-half">
                  <figure class="image">
                    <div class="columns">
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-scatter.png" alt="Scatter plot showing performance distribution">
                      </div>
                      <div class="column is-half">
                        <img src="static/images/pi0-FAST-in-the-wild-@-GRASP-bar.png" alt="Chart showing overall performance statistics">
                      </div>
                    </div>
                    <figcaption>Performance across 240+ trials (52% average progress)</figcaption>
                  </figure>
                </div>
                <div class="column is-half">
          <div class="content">
                    <p>Across our 240+ test trials, pi0 achieved varying degrees of success:</p>
                    <ul>
                      <li><strong>Complete Success:</strong> 38%</li>
                      <li><strong>Partial Success:</strong> 28%</li>
                      <li><strong>Complete Failure:</strong> 34%</li>
            </ul>
                    <p>We observed that performance varied significantly based on task type, environmental conditions, and most importantly, the phrasing of instructions.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
        </div>
        
        <!-- Task Specific Results -->
        <h3 class="title is-4">Task-Specific Performance</h3>
        
        <!-- Pick and Place -->
        <div class="box">
          <h4 class="title is-5">Pick-and-Place (44% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Familiar objects (e.g., pineapple toy, markers) - 90% success</li>
                  <li>Clear spatial targets ("in the pink bowl") - 85% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Large objects (black cube) - 35% success</li>
                  <li>Vague targets ("inside the bowl") - 40% success</li>
                  <li>Multi-object tasks ("Put all objects into the basket") - 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <figure class="image">
                <img src="static/images/pick_and_place_chart.jpg" alt="Chart showing pick and place performance">
              </figure>
            </div>
          </div>
        </div>
        
        <!-- Human Interaction -->
        <div class="box">
          <h4 class="title is-5">Human Interaction (53.5% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Strengths:</strong></p>
                <ul>
                  <li>Object handovers - 46.67% success</li>
                  <li>Following a moving human - 65% success</li>
                </ul>
                <p><strong>Weaknesses:</strong></p>
                <ul>
                  <li>Precision interactions ("Shake hands") - 30% success</li>
                  <li>Recovery after interruption - 40% success</li>
                </ul>
              </div>
            </div>
      
            <div class="column is-4">
              <div class="columns is-multiline">
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\success\human_3_Pick up the pineapple and give it to the programmer.mp4" type="video/mp4">
      </video>
                  <p class="is-size-6 has-text-centered">"Give the pineapple to the programmer"</p>
    </div>
                <div class="column is-12">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static\videos\failure\human_7_Pick up the whiteboard eraser and give it to the programmer.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Give the whiteboard eraser to the programmer"</p>
  </div>
              </div>
            </div>

          </div>
        </div>
        

        <!-- Coffee Machine -->
        <div class="box">
          <h4 class="title is-5">Coffee Machine (8.00% Success)</h4>
          <div class="columns">
            <div class="column is-8">
              <div class="content">
                <p><strong>Capsule Coffee Machine:</strong></p>
                <ul>
                  <li>Close the capsule lid of coffee machine - 50% success</li>
                  <li>Pick up the capsule from the coffee machine - 0% success</li>
                  <li>Place the capsule into the coffee machine - 0% success</li>

                </ul>
                <p><strong>Espresso Coffee Machine:</strong></p>
                <ul>
                  <li>Pick up the coffee portafilter - 0% success</li>
                  <li>Pour the coffee into the cup - 0% success</li>
                  <li>Pick up the silver milk frothing pitcher- 33% success</li>
                </ul>
              </div>
            </div>
            <div class="column is-4">
              <div class="columns">
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/failure/coffee_failure.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Place the capsule into the coffee machine"</p>
                </div>
                <div class="column is-half">
                  <video autoplay muted loop playsinline width="100%" height="250px" style="border-radius: 10px;">
                    <source src="static/videos/right_Close_the_capsule_lid_of_the_coffee_machine.mp4" type="video/mp4">
                  </video>
                  <p class="is-size-6 has-text-centered">"Close the capsule lid of the coffee machine"</p>
                </div>
              </div>
            </div>
          </div>
        </div>
        
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>pi0 Evaluation</strong> by <a href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">GRASP Lab</a>, University of Pennsylvania.
      </p>
      
      <p>
        The website template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
</footer>

</body>
</html>
